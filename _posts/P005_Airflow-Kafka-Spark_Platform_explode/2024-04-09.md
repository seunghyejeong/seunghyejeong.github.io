---

---
## Project Name: kubernetes 권한 생성, kafka 마운트 , 토픽생성 
#### Date: 2024-04-09 09:20 
#### Tag: #kubernetesuser #airflowDockerfile0409
---
# Contents:

1.  airflow namespace 생성
2. user account 생성
    - private key 생성
```
openssl genrsa -out airflow.key 2048
```
    
    - crt 생성을 위한 csr 
```
openssl req -new -key test-user.key -out test-user.csr
```

- csr를 위한 cert 생성 
```
cat test-user.csr | base64 -w 0
```

```
LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ25EQ0NBWVFDQVFBd1Z6RUxNQWtHQTFVRUJoTUNTMUl4RXpBUkJnTlZCQWdNQ2xOdmJXVXRVM1JoZEdVeApJVEFmQmdOVkJBb01HRWx1ZEdWeWJtVjBJRmRwWkdkcGRITWdVSFI1SUV4MFpERVFNQTRHQTFVRUF3d0hZV2x5ClpteHZkekNDQVNJd0RRWUpLb1pJaHZjTkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNZkp0R1hqTmpPeHM2VlIKWmJVaURrRGJtZ0pFenRBVzFjSkpZaldad3BFSjREYjNkRllTZUZXSmE3WWxTUi95SS9WbHd3MHpjSVd3eHR1RApKdnRlRGhvakNwZ0sxM0oxNmJ2TXFpdVlKTEtOLzlpUUZWMnMxSS8rSi9ta3RoTjJQMGJvT3UzYWg2dTZHVEdKClhIc3k0T09RWHZHcjUxM2EwYVdnWVFENzJTYlplZFM5NHFWZG1BcDY4T2I5cWw0azVxaDVEbzFYaEZ1dkVFMG4KbStneUlKM3BhS3RSakI1aitPNHVKRkJ3UHFYSkozTFpoc3RUVHpMOXFJdDFzZ1htdnRNanF4Mm93UHVRR25DMgpyQ0E4TTFURFVkVzFFZXJtbk9OUzNtUExmZmhwd2NHdC9VUGFaOGM4Y0xGTy9yVStzUXcrQU8rdUQzYjFmYlR1CmxTUVRmNlVDQXdFQUFhQUFNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUE0Mkl6Qlpsc2w5MzNneE82UGRsSTEKOC9qK054VnFEZzdaRXNIV04wYUQvSXFiS0JDa1dlN0F3cUFYcnFCK2JLbmY2QTUyRFA3UVZLcXFLV2wxM3JyTwpHSE5leWJ5SHFzRE44aVExWFdLVWRqS2VGZkZ1MTFmODNIdEhrUFlla3ZJeUM0SnZmczdRQnlOQk5PbU9RZHloClVSWHRLeEgzZTV1VllFWWljUjZLYkFkOHg0SEhnRnhqcFYrcllpTHlHUDdWTkcveG9IYnc5ZzRuNEkzNzZ6TWwKcHlrMU1ZNjhUUkI4UDZOR3VySkhNUkFYenZXbUovSTZtMGljaEtZUkExQTNjTHhrd1dXSjAzT21DS0Z1MllZdwpLMXBxcmhaZWZ3NTAzL2NEUmxoc3M5U0hBVXNqSVp1d1ZEbXd4MG1aNjZmd21BSW4yT1MxN0h0OWk0cktEejRPCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
```

- airflow-csr.yaml
```yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: airflow
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ25EQ0NBWVFDQVFBd1Z6RUxNQWtHQTFVRUJoTUNTMUl4RXpBUkJnTlZCQWdNQ2xOdmJXVXRVM1JoZEdVeApJVEFmQmdOVkJBb01HRWx1ZEdWeWJtVjBJRmRwWkdkcGRITWdVSFI1SUV4MFpERVFNQTRHQTFVRUF3d0hZV2x5ClpteHZkekNDQVNJd0RRWUpLb1pJaHZjTkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNZkp0R1hqTmpPeHM2VlIKWmJVaURrRGJtZ0pFenRBVzFjSkpZaldad3BFSjREYjNkRllTZUZXSmE3WWxTUi95SS9WbHd3MHpjSVd3eHR1RApKdnRlRGhvakNwZ0sxM0oxNmJ2TXFpdVlKTEtOLzlpUUZWMnMxSS8rSi9ta3RoTjJQMGJvT3UzYWg2dTZHVEdKClhIc3k0T09RWHZHcjUxM2EwYVdnWVFENzJTYlplZFM5NHFWZG1BcDY4T2I5cWw0azVxaDVEbzFYaEZ1dkVFMG4KbStneUlKM3BhS3RSakI1aitPNHVKRkJ3UHFYSkozTFpoc3RUVHpMOXFJdDFzZ1htdnRNanF4Mm93UHVRR25DMgpyQ0E4TTFURFVkVzFFZXJtbk9OUzNtUExmZmhwd2NHdC9VUGFaOGM4Y0xGTy9yVStzUXcrQU8rdUQzYjFmYlR1CmxTUVRmNlVDQXdFQUFhQUFNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUE0Mkl6Qlpsc2w5MzNneE82UGRsSTEKOC9qK054VnFEZzdaRXNIV04wYUQvSXFiS0JDa1dlN0F3cUFYcnFCK2JLbmY2QTUyRFA3UVZLcXFLV2wxM3JyTwpHSE5leWJ5SHFzRE44aVExWFdLVWRqS2VGZkZ1MTFmODNIdEhrUFlla3ZJeUM0SnZmczdRQnlOQk5PbU9RZHloClVSWHRLeEgzZTV1VllFWWljUjZLYkFkOHg0SEhnRnhqcFYrcllpTHlHUDdWTkcveG9IYnc5ZzRuNEkzNzZ6TWwKcHlrMU1ZNjhUUkI4UDZOR3VySkhNUkFYenZXbUovSTZtMGljaEtZUkExQTNjTHhrd1dXSjAzT21DS0Z1MllZdwpLMXBxcmhaZWZ3NTAzL2NEUmxoc3M5U0hBVXNqSVp1d1ZEbXd4MG1aNjZmd21BSW4yT1MxN0h0OWk0cktEejRPCi0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  expirationSeconds: 86400  # one day
  usages:
  - client auth
```

- kubectl get csr airflow
```
NAME      AGE   SIGNERNAME                            REQUESTOR          REQUESTEDDURATION   CONDITION
airflow   2s    kubernetes.io/kube-apiserver-client   kubernetes-admin   24h                 Pending
```

```
kubectl certificate approve airflow

certificatesigningrequest.certificates.k8s.io/airflow approved
```

- crt파일이 생김 
```
kubectl get csr airflow -o jsonpath='{.status.certificate}' | base64 -d > airflow.crt
```

```
kubectl config set-credentials airflow --client-key=airflow.key --client-certificate=airflow.crt
```

```
kubectl config set-context airflow-context --cluster=kubernetes --user=airflow
```

```
kubectl config get-contexts
```

```
kubectl config use-context airflow-context
```


- kubernetes 접속 
```
kubectl config get-contexts
CURRENT   NAME              CLUSTER      AUTHINFO   NAMESPACE
          airflow-context   kubernetes   airflow
*         cluster1          cluster1     cluster1
```


3. clusterrole 생성 및 배포  
```yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: airflow-admin-cluster-role
rules:
- apiGroups: [""]
  verbs: ["*"]
  resources: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: airflow-admin-cluster-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: airflow-admin-cluster-role
subjects:
- kind: User
  name: airflow
  namespace: airflow

```

/home/airflow/.local/bin

# ? Fernet key
```
echo Fernet Key: $(kubectl get secret --namespace airflow airflow-fernet-key -o jsonpath="{.data.fernet-key}" | base64 --decode)
Fernet Key: V1lJOGFEQWU2Q1RudVNyaVdVMEIya2VRaTJBZWVQUkc=

```

```
from datetime import datetime as dt
from datetime import timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
import pendulum

local_tz = pendulum.timezone("Asia/Seoul")

default_args = {
    'owner': 'admin',
    'depends_on_past': False,
    'start_date': dt(2024, 1, 1),
    'retries': 1,
    'provide_context': True,
    'retry_delay': timedelta(minutes=1)
}

with DAG('pipeline', default_args=default_args, schedule_interval=None) as dag:
    deploy_kafka_topic = BashOperator(
        task_id='deploy_kafka_topic',
        bash_command='kubectl apply -f /opt/airflow/kafka/kafka-topic.yaml',
        dag=dag,
        )
    
    deploy_kafka_topic
```

# kubectl permission denied

유저를 생성 했음에도 불구하고 쓸 수 없었음
이유: kubectl이 airflow container 안에서도 쓸 수 있어야하기 때문.

Dockerfile을 수정하여 kubectl이 /usr/local/bin에 설치되도록 했다.
    처음에는 webserver에만 설치를 해서 kubectl을 사용하지 못했는데, 전 컨테이너에 다 설치하도록 이미지를 바꿈.
```Dockerfile
FROM apache/airflow:2.8.4-python3.10

ENV AIRFLOW_HOME=/opt/airflow
ENV JAVA_HOME=/usr/lib/jvm/jdk1.8.0_401
ENV PATH="$PATH:${AIRFLOW_HOME}/bin:${JAVA_HOME}/bin"

USER root

RUN mkdir /usr/lib/jvm && mkdir ./.kube
COPY jdk-11.0.0.1.tar.gz /opt/airflow
RUN tar xvf jdk-11.0.0.1.tar.gz -C /usr/lib/jvm \
	&& rm jdk-11.0.0.1.tar.gz

===
RUN sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" && sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
=== 이부분

RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        vim \
        wget \
        net-tools \
        dnsutils \
        iputils-ping \
        netcat-openbsd \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

#RUN curl -o  /opt/airflow/jars/kafka-clients-3.5.1.jar https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.1/kafka-clients-3.5.1.jar
#RUN curl -o /opt/airflow/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar
#RUN curl -o /opt/airflow/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar
#RUN curl -o /opt/airflow/jars/commons-pool2-2.11.0.jar https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.0/commons-pool2-2.11.0.jar

USER airflow
COPY ./requirements.txt /
RUN pip install --no-cache-dir "apache-airflow==${AIRFLOW_VERSION}" -r /requirements.txt

```

kafka의 실행 파일이 컨테이너 내부에 위치하고 있으므로 모든 pod가 같은 경로의 같은 파일을 접근 할 수 있어야헀다 즉  -> *내조건은 모든 컨테이너에 kafka 파일이 공유 되어야하고 (dags처럼) 모든 파드에 kubectl이 가능 해야 했다.*
- container 모든 구역에 volume을 default로 잡아주는 custom-values.yaml을 수정 함.
- 폴더는 각 container에 다 생겼다. 그러나 webserver에만 파일이 들어와있고 나머지 파드에는 파일이 없었다.
- 다른점은 webserver만 1000 user이고 나머지는 root 였다
 컨테이너 목록
   - worker scheduler createUserJob migrateDatabaseJob triggerer dagProcessor, redis
- 일단 kafka를 airflow안으로 폴더를 옮겼다
    - dockerfile로 Volume을 해도 안됐고
    - 컨테이너 마다 extraVolume을 kafka-volume(volumes)로 지정해도 안됐다
    - volumes, volumesmount를 이용해도 안됐다 
    - 아래의 securityContext를 써야 하나 고민..
```
# User and group of airflow user
uid: 50000
gid: 0

# Default security context for airflow (deprecated, use `securityContexts` instead)
securityContext: {}
#  runAsUser: 50000
#  fsGroup: 0
#  runAsGroup: 0

# Detailed default security context for airflow deployments
securityContexts:
  pod: {}
  containers: {}
```

- [?] 근데 진짜 얼떨결에 해결. volumes와 volumesmount중 volumemount를 주석 처리 후 배포했더니 너무 잘됐다. 이유가 뭐지 ? 
 - [k] dags를 sc와 연결? -> 아님 있었음
- [k] 아래 두 개를 새롭게 연결하긴 함. 
 ```
 triggerer:
  persistence:
    enabled: true
    size: 5Gi
    storageClassName: cp-storageclass
redis:
  persistence:
    enabled: true
    size: 5Gi
    storageClassName: cp-storageclass
```

🤔 그런데 내가 막 이상하게 dags.mountPath를 `/opt/airflow/kafka`로 해놓았더니 모든 dag파일이 여기에 마운트되더라;;
### custom-values.yaml
```yaml
images:
  airflow:
    repository: seunghyejeong/airflow_v2
    tag: "1.0"
    digest: ~
    pullPolicy: IfNotPresent

# Load Examples
extraEnv: |
  - name: AIRFLOW__CORE__LOAD_EXAMPLES
    value: 'False'
  - name: AIRFLOW__CORE__DEFAULT_TIMEZONE
    value: 'Asia/Seoul'

#set executor
executor: "KubernetesExecutor"


volumes:
  - name: kafka-volume
    hostPath:
      path: /home/ubuntu/kafka
        #volumeMounts:
         #  - name: kafka-volume
         #    mountPath: /opt/airflow/kafka
         #    readOnly: false

# Webserver configure
webserver:
  defaultUser:
    enabled: true
    role: Admin
    username: admin
    email: admin@example.com
    firstName: admin
    lastName: admin
    password: admin
  service:
    type: NodePort
    ports:
      - name: airflow-ui
        port: 8080
        targetPort: 8080
        nodePort: 31151
  nodeSelector:
    node-role.kubernetes.io/control-plane: ""
  extraVolumeMounts:
    - name: airflow-dags
      mountPath: /opt/airflow/dags
    - name: kubeconfig-volume
      mountPath: /opt/airflow/.kube/kubeconfig
      readOnly: true
  extraVolumes:
    - name: airflow-dags
      persistentVolumeClaim:
        claimName: airflow-dags
    - name: kubeconfig-volume
      secret:
        secretName: kubeconfig-secret

workers:
  replicas: 1
  nodeSelector:
    node-role.kubernetes.io/control-plane: "
      "
webserverSecretKey: a717f95983a54fb8a822c40ba806cfc8

# bind w strogaeClass
dags:
  persistence:
    enabled: true
    storageClassName: cp-storageclass
    accessMode: ReadWriteMany
    size: 5Gi
workers:
  persistence:
    enabled: true
    storageClassName: cp-storageclass
    size: 5Gi
logs:
  persistence:
    enabled: true
    storageClassName: cp-storageclass
triggerer:
  persistence:
    enabled: true
    size: 5Gi
    storageClassName: cp-storageclass
redis:
  persistence:
    enabled: true
    size: 5Gi
    storageClassName: cp-storageclass

```

# KAFKA_STREAMING

배포된 kafka, zookeeper 조회 

```bash
kubectl get strimzipodsets -n kafka
NAME             PODS   READY PODS   CURRENT PODS   AGE
bami-kafka       1      1            1              29h
bami-zookeeper   1      1            1              29h
```

strimzi로 구성된 network 조회 
```bash
ubuntu@bami-node1:~$ kubectl get kafka -n kafka bami -o jsonpath={.status.listeners} | jq
[
  {
    "addresses": [
      {
        "host": "192.168.0.24",
        "port": 31207
      }
    ],
    "bootstrapServers": "192.168.0.24:31207",
    "name": "plain"
  },
  {
    "addresses": [
      {
        "host": "192.168.0.24",
        "port": 32507
      }
    ],
    "bootstrapServers": "192.168.0.24:32507",
    "name": "tls"
  }

```

- 브로커 정보 조회
```
ubuntu@bami-node1:~/airflow$ kubectl exec -ti kafka-client -n kafka -- bash
[appuser@kafka-client ~]$ kafka-broker-api-versions --bootstrap-server=bami-kafka-plain-bootstrap.svc.kafka:9092
[2024-04-09 07:49:45,216] WARN Couldn't resolve server bami-kafka-plain-bootstrap.svc.kafka:9092 from bootstrap.servers as DNS resolution failed for bami-kafka-plain-bootstrap.svc.kafka (org.apache.kafka.clients.ClientUtils)
Exception in thread "main" org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:101)
	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:60)
	at kafka.admin.BrokerApiVersionsCommand$AdminClient$.create(BrokerApiVersionsCommand.scala:284)
	at kafka.admin.BrokerApiVersionsCommand$AdminClient$.create(BrokerApiVersionsCommand.scala:267)
	at kafka.admin.BrokerApiVersionsCommand$AdminClient$.create(BrokerApiVersionsCommand.scala:265)
	at kafka.admin.BrokerApiVersionsCommand$.createAdminClient(BrokerApiVersionsCommand.scala:77)
	at kafka.admin.BrokerApiVersionsCommand$.execute(BrokerApiVersionsCommand.scala:59)
	at kafka.admin.BrokerApiVersionsCommand$.main(BrokerApiVersionsCommand.scala:54)
	at kafka.admin.BrokerApiVersionsCommand.main(BrokerApiVersionsCommand.scala)
[appuser@kafka-client ~]$ kafka-broker-api-versions --bootstrap-server=bami-kafka-plain-bootstrap.kafka.svc:9092
192.168.0.24:30774 (id: 0 rack: null) -> (
	Produce(0): 0 to 9 [usable: 9],
	Fetch(1): 0 to 15 [usable: 15],
	ListOffsets(2): 0 to 8 [usable: 8],
	Metadata(3): 0 to 12 [usable: 12],
	LeaderAndIsr(4): 0 to 7 [usable: 7],
	StopReplica(5): 0 to 4 [usable: 4],
	UpdateMetadata(6): 0 to 8 [usable: 8],
	ControlledShutdown(7): 0 to 3 [usable: 3],
	OffsetCommit(8): 0 to 8 [usable: 8],
	OffsetFetch(9): 0 to 8 [usable: 8],
	FindCoordinator(10): 0 to 4 [usable: 4],
	JoinGroup(11): 0 to 9 [usable: 9],
	Heartbeat(12): 0 to 4 [usable: 4],
	LeaveGroup(13): 0 to 5 [usable: 5],
	SyncGroup(14): 0 to 5 [usable: 5],
	DescribeGroups(15): 0 to 5 [usable: 5],
	ListGroups(16): 0 to 4 [usable: 4],
	SaslHandshake(17): 0 to 1 [usable: 1],
	ApiVersions(18): 0 to 3 [usable: 3],
	CreateTopics(19): 0 to 7 [usable: 7],
	DeleteTopics(20): 0 to 6 [usable: 6],
	DeleteRecords(21): 0 to 2 [usable: 2],
	InitProducerId(22): 0 to 4 [usable: 4],
	OffsetForLeaderEpoch(23): 0 to 4 [usable: 4],
	AddPartitionsToTxn(24): 0 to 4 [usable: 4],
	AddOffsetsToTxn(25): 0 to 3 [usable: 3],
	EndTxn(26): 0 to 3 [usable: 3],
	WriteTxnMarkers(27): 0 to 1 [usable: 1],
	TxnOffsetCommit(28): 0 to 3 [usable: 3],
	DescribeAcls(29): 0 to 3 [usable: 3],
	CreateAcls(30): 0 to 3 [usable: 3],
	DeleteAcls(31): 0 to 3 [usable: 3],
	DescribeConfigs(32): 0 to 4 [usable: 4],
	AlterConfigs(33): 0 to 2 [usable: 2],
	AlterReplicaLogDirs(34): 0 to 2 [usable: 2],
	DescribeLogDirs(35): 0 to 4 [usable: 4],
	SaslAuthenticate(36): 0 to 2 [usable: 2],
	CreatePartitions(37): 0 to 3 [usable: 3],
	CreateDelegationToken(38): 0 to 3 [usable: 3],
	RenewDelegationToken(39): 0 to 2 [usable: 2],
	ExpireDelegationToken(40): 0 to 2 [usable: 2],
	DescribeDelegationToken(41): 0 to 3 [usable: 3],
	DeleteGroups(42): 0 to 2 [usable: 2],
	ElectLeaders(43): 0 to 2 [usable: 2],
	IncrementalAlterConfigs(44): 0 to 1 [usable: 1],
	AlterPartitionReassignments(45): 0 [usable: 0],
	ListPartitionReassignments(46): 0 [usable: 0],
	OffsetDelete(47): 0 [usable: 0],
	DescribeClientQuotas(48): 0 to 1 [usable: 1],
	AlterClientQuotas(49): 0 to 1 [usable: 1],
	DescribeUserScramCredentials(50): 0 [usable: 0],
	AlterUserScramCredentials(51): 0 [usable: 0],
	DescribeQuorum(55): UNSUPPORTED,
	AlterPartition(56): 0 to 3 [usable: 3],
	UpdateFeatures(57): 0 to 1 [usable: 1],
	Envelope(58): 0 [usable: 0],
	DescribeCluster(60): 0 [usable: 0],
	DescribeProducers(61): 0 [usable: 0],
	UnregisterBroker(64): UNSUPPORTED,
	DescribeTransactions(65): 0 [usable: 0],
	ListTransactions(66): 0 [usable: 0],
	AllocateProducerIds(67): 0 [usable: 0],
	ConsumerGroupHeartbeat(68): UNSUPPORTED
```

- [b] ref
> [messaging반복하기 ](https://velog.io/@yoojh5099/Kafka-Strimzi-Operator)
> [strimzi로 배포 및 실행하기](https://velog.io/@yoojh5099/Kafka-Strimzi-Operator)

- [?] KEDA 사용?

```
kafka-console-producer --topic kafka-topic --bootstrap-server 192.168.0.24:31207
```