---

---
## Project Name:  install 
#### Date: 2024-04-01 10:34 
#### Tag:
---
# Contents:

- [i] IP 정보 

 - [b] REF 
 > [Spark on Kuberenetes](https://velog.io/@newnew_daddy/spark06)
> [airflow와 Spark를 Kubernetes로 연동하기](https://mightytedkim.tistory.com/43)
> [아키텍쳐](https://ssup2.github.io/blog-software/docs/theory-analysis/spark-on-kubernetes/)


| bami-node1 | Ubuntu Server 22.04 LTS | 172.16.1.102, 125.6.37.91   |
| ---------- | ----------------------- | --------------------------- |
| bami-node2 | Ubuntu Server 22.04 LTS | 172.16.1.15, 133.186.159.15 |
| bami-node3 | Ubuntu Server 22.04 LTS | 172.16.1.26, 133.186.216.17 |

```
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC1y010nwsnzfBpupTysNEIOySkIe6pJovYitbbM43YH8oh9FS3Z6ibYxFofoVhmdlIFICpMl6Jt0ryEvVetcJ7jZL1zfLAA+D/IblIwaQx7QmrKhcEwu7Dgmfcmw0LVEJ+HmX+9gtOoZKAwTX3p4TQGBqmQ0HHRhLbiLB+mDNyNJARVfqX2rCA6Qp5aV0/4xAEuw/EPpmXyXuGwdSGH7LvR4OQ3OxgC7t7I50g5wPf9DLmhMr/y3TzaF2RsSczhdut1u8FlFBjBVGRlUfQ+16NgUIaExlezqmZPSu2gJsPCaf8ug0ZyC5fHVBf12QgHvu1R0nPDXfzKEjBvjfgp3J7bFUtcBB/kLETA8eBmggTVpOgUSVNfzVkILax5pmQzwrfi/ahs9L+6sFTiz4lU4oinEJuBAYrnNcdrpIL4IfE0GFewbKe0E076JO9WkFVj1RC+dQzOW7Z2Ox4v6MICBFErQWfLnOMCT+v71p2fnzi6fd+saz3ouxTCJWHt1ktWTs= ubuntu@bami-node1
```


/home/share/nfs 172.16.1.102(rw,no_root_squash,async)
/home/share/nfs 172.16.1.15(rw,no_root_squash,async)
/home/share/nfs 172.16.1.26(rw,no_root_squash,async)


# Version

Python 3.10.12
spark 3.5.1
scala 2.13
java 11
airflow 2.8.4-python3.10


# Airflow 
> helm install

```bash
helm repo add apache-airflow https://airflow.apache.org
```

```
helm upgrade --install airflow apache-airflow/airflow --namespace airflow --create-namespace
```

```bash
helm upgrade airflow apache-airflow/airflow --namespace airflow -f custom_values.yaml
```

```bash
helm delete airflow --namespace airflow
```


# Spark



```bash
helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator
helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator

install my-release spark-operator/spark-operator --namespace spark-operator --create-namespace
```

```
helm install my-release spark-operator/spark-operator --namespace spark-operator --create-namespace
```

- spark
```bash
wget https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3-scala2.13.tgz
```

- java
```
sudo apt-get install openjdk-11-jdk
```

- env
```
export SPARK_HOME=/home/ubuntu/spark >> .bashrc
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/ >> .bashrc
export PATH=$PATH:$SPARK_HOME/bin:/$SPARK_HOME/bin >> .bashrc
source .bashrc
```

- pyspark
```
sudo apt install python3-pip
```

```
pip install python3-pip
```


# Kafka
- [b] REF
> [google kafka statefulset](https://cloud.google.com/kubernetes-engine/docs/tutorials/stateful-workloads/kafka?hl=ko)
> [kafka를 pod로 배포할 때 고려사항](https://tommypagy.tistory.com/424)
> [DB와 연결하여 k8s에 배포한 사례](https://engineering.linecorp.com/ko/blog/line-shopping-platform-kafka-mongodb-kubernetes)
> [kafka의 토픽과 파티션](https://medium.com/naverfinancial/k8s%EC%97%90%EC%84%9C-%EC%B9%B4%ED%94%84%EC%B9%B4-%EB%A6%AC%EB%B0%B8%EB%9F%B0%EC%8B%B1-%EB%B0%A9%EC%A7%80%ED%95%98%EA%B8%B0-c9452a239c28)
https://medium.com/analytics-vidhya/apache-kafka-architecture-getting-started-with-apache-kafka-771d69ac6cef

- statefulset으로 배포

```
              value: "server.1=zookeeper-0.zookeeper:2888:3888 \
                      server.2=zookeeper-1.zookeeper:2888:3888 \
                      server.3=zookeeper-2.zookeeper:2888:3888"
```