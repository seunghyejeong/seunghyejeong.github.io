---

---
## Project Name: Helm Chartë¡œ ì„¤ì¹˜í•˜ê¸° 
#### Date: 2024-02-15 09:45 
#### Tag:
---
# Contents:

- [i] ì‚¬ì „ ì„¤ì¹˜ 
- Docker

- [!] Version
> Helm airflow-1.12.0 

# Pre

- workspace ë§Œë“¤ê¸° 
```bash
mkdir $HOME/airflow && cd airflow 
```

- Dag ì €ì¥ì†Œ ë§Œë“¤ê¸°
```bash
mkdir dags && cd dags
```

- kafkatest.py ìƒì„±í•˜ê¸°
```bash
vi kafkatest.py
```

```bash
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from kafka import KafkaProducer, KafkaConsumer
from kafka.admin import KafkaAdminClient, NewTopic
from datetime import datetime
from airflow.models import TaskInstance
from kafka.errors import TopicAlreadyExistsError

# Define Kafka broker
bootstrap_servers = '133.186.240.216:19092'
topic_name = 'airflow-topic'

# Function to check if Kafka is connected
def check_connected():
    try:
        producer = KafkaProducer(bootstrap_servers=bootstrap_servers)
        consumer = KafkaConsumer(topic_name, bootstrap_servers=bootstrap_servers)
        return True
    except Exception as e:
        print(f"Error connecting to Kafka: {str(e)}")
        return False
        
# Function to create Kafka topic
def create_topic():
    admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)
    existing_topics = admin_client.list_topics()
    # í† í”½ì´ ì´ë¯¸ ìƒì„± ë˜ì–´ ìˆë‹¤ë©´ ìƒì„± ë˜ì–´ ìˆëŠ” í† í”½ì„ ê°€ì ¸ì˜¨ë‹¤.
    if topic_name in existing_topics:
        print(f"Topic '{topic_name}' already exists.")
    else:
        topic = NewTopic(name=topic_name, num_partitions=1, replication_factor=1)
        try:
            admin_client.create_topics([topic])
            print(f"Topic '{topic_name}' created successfully.")
        except TopicAlreadyExistsError:
            print(f"Topic '{topic_name}' already exists.")


# Function to produce a message to Kafka
def produce_message():
    producer = KafkaProducer(bootstrap_servers=bootstrap_servers,linger_ms=20)#ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹Œ 20ì´ˆ ê°„ê²©ìœ¼ë¡œ ë©”ì„¸ì§€ë¥¼ ì „ì†¡í•œë‹¤.
    message = f"Hello from Airflow. Message produced at {datetime.now()}"
    producer.send(topic_name, message.encode())
    producer.flush()

# Function to consume a message from Kafka
def consume_message(**context):
    consumer = KafkaConsumer(topic_name, bootstrap_servers=bootstrap_servers, auto_offset_reset='earliest')
    try:
        message = next(consumer)
        print(f"Message consumed: {message.value.decode()}")
        # Mark the task as success
        context['task_instance'].log.info("Message consumed successfully")
        context['task_instance'].state = "success"
    except StopIteration:
        # No message available, mark the task as failed
        context['task_instance'].log.info("No message available to consume")
        context['task_instance'].state = "failed"
    finally:
        # Close the consumer to release resources
        consumer.close()

# Function to finish
def finish():
    print("All tasks completed.")

# Define the DAG
default_args = {
    'owner': 'admin',
    'depends_on_past': False,
    'start_date': datetime(2024, 2, 7),
    'retries': 1,
}

dag = DAG(
    'airflow_kafka_interworking',
    default_args=default_args,
    description='DAG for testing interworking of Airflow and Kafka',
    schedule_interval=None,
)

# Define tasks
create_topic_task = PythonOperator(
    task_id='create_topic',
    python_callable=create_topic,
    dag=dag,
)

check_connected_task = PythonOperator(
    task_id='check_connected',
    python_callable=check_connected,
    dag=dag,
)

produce_message_task = PythonOperator(
    task_id='produce_message',
    python_callable=produce_message,
    dag=dag,
)

consume_message_task = PythonOperator(
    task_id='consume_message',
    python_callable=consume_message,
    dag=dag,
)

finish_task = PythonOperator(
    task_id='finish',
    python_callable=finish,
    dag=dag,
)

# Define task dependencies
check_connected_task >> create_topic_task  >> produce_message_task >> consume_message_task >> finish_task
```


# Custom Image ë§Œë“¤ê¸°
## ğŸ˜‚
> [[workflow(240215)#Dockerfile..ğŸ§]] ì— ë’¤ì´ì–´ ì‘ì„± í–ˆìŒ..

- requirements.txt ì‘ì„±í•˜ê¸° 

```txt
pandas~=2.0.3
requests~=2.31.0
selenium~=4.17.2
beautifulsoup4~=4.12.3
lxml~=5.1.0
kafka-python~=2.0.2
apache-airflow-providers-apache-kafka~=1.3.1
confluent-kafka~=2.3.0
```

- Dockerfile
```dockerfile
FROM apache/airflow:2.8.1  
COPY requirements.txt .

USER root
RUN apt-get update \
  && apt-get install -y --no-install-recommends \
         gcc \
         heimdal-dev \
         default-libmysqlclient-dev \
         pkg-config \
  && apt-get autoremove -yqq --purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

USER airflow
RUN pip install --upgrade --no-deps --force-reinstall -r requirements.txt
RUN pip uninstall -y argparse  

EXPOSE 8080  
EXPOSE 8793  
EXPOSE 5555
```

```bash
docker build --no-cache -t seunghyejeong/airflow:1.0 .
docker push seunghyejeong/airflow:1.0
```

# Install by helm (default)

```bash
helm repo add apache-airflow https://airflow.apache.org
helm pull apache-airflow/airflow
helm upgrade --install airflow apache-airflow/airflow --namespace airflow --create-namespace
helm delete airflow --namespace airflow
```

# Custom Values 

1. Custom Image ì‚¬ìš© 
```yaml
images:
  airflow:
    repository: seunghyejeong/airflow
    tag: "1.0"
    digest: ~
    pullPolicy: IfNotPresent
```

2. Example ì˜ˆì œ ë¶ˆëŸ¬ì˜¤ê¸°
```yaml
extraEnv: |
  - name: AIRFLOW__CORE__LOAD_EXAMPLES
    value: 'True'
```

3. Logë¥¼ Storage Classì— ì¶”ê°€
```yaml
logs:
  persistence:
    enabled: true
    storageClassName: cp-stroageclass
dags:
  persistence:
    enabled: true
    storageClassName: cp-storageclass
```

4. user ì„¤ì •
```yaml
webserver:
  defaultUser:
    enabled: true
    role: Admin
    username: admin
    email: admin@example.com
    firstName: admin
    lastName: user
    password: admin
```


## Dags files shared by PVC

- [b] REF
> [moung ë°©ë²•](https://velog.io/@kunheekimkr/Kubernetes-%ED%99%98%EA%B2%BD-%EC%9C%84%EC%97%90-Airflow-%EB%B0%B0%ED%8F%AC%ED%95%98%EA%B8%B0)
> [pvc/pv mount](https://swalloow.github.io/airflow-on-kubernetes-1/)

- PV, PVC ìƒì„± 

> PVëŠ” í•„ìš” ì—†ì—ˆë„¤ 
~~apiVersion: v1
kind: PersistentVolume
metadata:
  name: airflow-dags
  namespace: airflow
  labels:
    heritage: Helm
    release: airflow
    tier: airflow
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 172.16.11.19
    path: /home/share/nfs
  persistentVolumeReclaimPolicy: Retain~~ 
  
```yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    volume.beta.kubernetes.io/storage-provisioner: cp-nfs-provisioner
    volume.kubernetes.io/storage-provisioner: cp-nfs-provisioner 
  name: airflow-dags
  namespace: airflow
  labels:
    heritage: Helm
    release: airflow
    tier: airflow
spec:
  storageClassName: "cp-storageclass"
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  volumeMode: Filesystem
```

- Custom Values.yaml ìˆ˜ì • 

```yaml
dags:
  persistence:
    enabled: true
    existingClaim: "airflow-dags"
    accessMode: ReadWriteMany
    size: 5Gi
```

# Custom Image, values ì ìš© í•˜ì—¬ ì„¤ì¹˜í•˜ê¸°


>ì„¤ì¹˜
```bash
helm upgrade --install airflow apache-airflow/airflow --namespace airflow --create-namespace -f custom_values.yaml
```

> ì‚­ì œ 
```bash
helm delete airflow --namespace airflow 
```

> custom_values.yaml

```yaml
airflowHome: /home/ubuntu/airflow

# Custom Images (include PyPI Packages)
images:
  airflow:
    repository: seunghyejeong/airflow
    tag: "1.0"
    pullPolicy: IfNotPresent
# Load Examples
extraEnv: |
  - name: AIRFLOW__CORE__LOAD_EXAMPLES
    value: 'True'

# logs bind with stroageClass
logs:
  persistence:
    enabled: true
    storageClassName: cp-stroageclass

# Webserver user configure
webserver:
  defaultUser:
    enabled: true
    role: Admin
    username: admin
    email: admin@example.com
    firstName: admin
    lastName: user
    password: admin            

# dags bind w strogaeClass 
dags:
  persistence:
    enabled: true
    existingClaim: "airflow-dags"
    accessMode: ReadWriteMany
    size: 5Gi

# ingress
ingress:
  enabled: true
  web:
    enabled: true
    annotations: {}
    path: "/"
    pathType: "ImplementationSpecific"
    host: airflow.nip.io
    ingressClassName: "nginx"

webserver:
  service:
    ports:
      - name: airflow-ui
        port: 80
        targetPort: 8080
---
webserver:
  service:
    type: NodePort
    ports:
      - name: airflow-ui
        port: 8080
        targetPort: 8080
        nodePort: 31151

```

```bash
helm install airflow apache-airflow/airflow --namespace airflow -f custom_values.yaml
```


# Dockerfile..ğŸ§

- [b] REF
> [ê³µì‹ Docsì˜ Airflow Dockerfile](https://airflow.apache.org/docs/docker-stack/build.html)

? 
```Dockerfile
FROM apache/airflow:2.6.1
USER root
RUN apt-get update \
  && apt-get install -y --no-install-recommends \
     	vim \
  && apt-get autoremove -yqq --purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*
USER "${AIRFLOW_UID:-50000}:0"
```

ã… ã… 
```Dockerfile
FROM apache/airflow:2.8.1
COPY requirements.txt /
USER root
RUN apt-get update \
  && apt-get install -y --no-install-recommends \
         vim \
  && apt-get autoremove -yqq --purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*
USER airflow
RUN pip install --no-cache-dir "apache-airflow==2.8.1" -r /requirements.txt
```

### ì—ëŸ¬ ë¡œê·¸
![[Pasted image 20240215153047.png]]

1. Chartì˜ Values.yamlì„ Original versionìœ¼ë¡œ ë°°í¬ 
- ë°°í¬ëœ `webserver`ì˜  image ë¥¼ ë´¤ëŠ”ë° Containerì—ì„œ 'airflow' CLIë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ ê°™ë‹¤.
```yaml
      containers:
      - args:
        - bash
        - -c
        - exec airflow webserver
        image: apache/airflow:2.8.1
        imagePullPolicy: IfNotPresent
      initContainers:
      - args:
        - airflow
        - db
        - check-migrations
        - --migration-wait-timeout=60
        image: apache/airflow:2.8.1
        imagePullPolicy: IfNotPresent

```

2. ì»¨í…Œì´ë„ˆ ì•ˆì— ë“¤ì–´ì™€ì„œ echo PATHë¥¼ í–ˆë”ë‹ˆ
![[Pasted image 20240215153237.png]]

### ê·¸ë˜ì„œ í˜¹ì‹œë‚˜ í™˜ê²½ ë³€ìˆ˜ .. ?
```Dockerfile
FROM apache/airflow:2.8.1
RUN echo $PATH
ENV PATH="$PATH:/home/airflow/.local/bin"
RUN echo $PATH
COPY requirements.txt /
RUN pip install --no-cache-dir "apache-airflow==2.8.1" -r /requirements.txt
```

*í–ˆë”ë‹ˆ ì„¤ì¹˜ê°€ ë˜ê¸´ í•¨;*

ë„ã…ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ã…‡ë“œë””ì–´ëì–´ ã…¡ã…¡ ì—´ë°›ì•„
![[Pasted image 20240215153507.png]]

*ì•„ home/ubuntu/airflowë‘ mountí•˜ë ¤ í–ˆë”ë‹ˆ ì—ëŸ¬ê°€ ìê¾¸ ë‚˜ì„œ ê·€ì°®ì•„ ê·¸ë˜ì„œ logë‘ homeë””ë ‰í† ë¦¬ëŠ” defulatë¡œ ì§„í–‰í• ë ¤ ã…* 

- ë˜ì ¸ì£¼ì—ˆìŠµë‹ˆë‹¤ 
```bash
kubectl cp ./dags/kafkatest.py airflow-webserver-b744c4466-8tpz8:/opt/airflow/dags/kafkatest.py -n airflow 
```


# ìµœìµœìµœìµœìµœìµœìµœìµœã…—ì¹˜ì¹˜ì¹˜ã…—ì¹˜ìµœì¢….zip

> 4.2 AirFlow Helm Catalog Repo ì¶”ê°€ > Helm Resource > airflow DIRì— ì˜®ê²¨ ë†“ìŒ


ê·¼ë° Example dagê°€ ì•ˆì˜¬ë¼ì˜¨ë‹´.. ì•”ë¡œë¼ë¼ë¦¬ã…ì–¼ã„´ã…‡

```yaml
#airflowHome: /home/ubuntu/airflow
# Custom Images (include PyPI Packages)
images:
  airflow:
    repository: seunghyejeong/airflow
    tag: "1.0"
    digest: ~
    pullPolicy: IfNotPresent

# Load Examples
extraEnv: |
  - name: AIRFLOW__CORE__LOAD_EXAMPLES
    value: 'True'

# Webserver configure
webserver:
  defaultUser:
    enabled: true
    role: Admin
    username: admin
    email: admin@example.com
    firstName: admin
    lastName: user
    password: admin
  service:
    type: NodePort
    ports:
      - name: airflow-ui
        port: 8080
        targetPort: 8080
        nodePort: 31151

# bind w strogaeClass
dags:
  persistence:
    enabled: true
    storageClassName: cp-storageclass
    accessMode: ReadWriteMany
    size: 5Gi
workers:
  persistence:
    enabled: true
    storageClassName: cp-storageclass
    size: 5Gi
logs:
  persistence:
    enabled: true
    storageClassName: cp-storageclass    
```