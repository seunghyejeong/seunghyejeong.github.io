---

---
## Project Name: airflow dag코드 
#### Date: 2024-03-11 09:16 
#### Tag:
---
# Contents:

- [b] REF
> [데이터 처리 방식](https://1ambda.gitbook.io/practical-data-pipeline/02-processing/2.4-stream/2.4.3-spark-streaming)



```
133.186.213.65
```

```
133.186.213.16
```

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from pyspark.sql import SparkSession
from kafka import KafkaProducer, KafkaConsumer
from kafka.admin import KafkaAdminClient, NewTopic
from datetime import datetime
from airflow.models import TaskInstance
from kafka.errors import TopicAlreadyExistsError

bootstrap_servers = '133.186.228.18:19092'
spark_master_url = '133.186.228.18:7077'
topic_name = 'devices'

# Function to check if Kafka is connected
def check_connected():
    try:
        admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)
        existing_topics = admin_client.list_topics()

        if topic_name in existing_topics:
            print(f"Topic '{topic_name}' already exists.")
        else:
            topic = NewTopic(name=topic_name, num_partitions=1, replication_factor=1)
            try:
                admin_client.create_topics([topic])
                print(f"Topic '{topic_name}' created successfully.")
            except TopicAlreadyExistsError:
                print(f"Topic '{topic_name}' already exists.")

        spark = SparkSession.builder.appName('first').master("spark://"+spark_master_url).getOrCreate()
        print("Spark is runniung :"+spark)

        print("############Successfully connected to Spark & Kafka!##################")
        return True

    except Exception as e:
        print(f"Error connecting to Kafka: {str(e)}")
        return False

from airflow import DAG
from airflow.operators.python import PythonOperator
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json
from kafka import KafkaProducer
from datetime import datetime
import json
import io
import avro.schema
from avro.io import DatumWriter
from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType

bootstrap_servers = '133.186.228.18:19092'
spark_master_url = '133.186.228.18:7077'
topic_name = 'devices'


def messeaging():

    try:
    # 메세지 보내기
        producer = KafkaProducer(bootstrap_servers=bootstrap_servers,linger_ms=20)
        message = {
            "id": 1,
            "name": "bami",
            "age": "7"
            }
        producer.send(topic_name, json.dumps(message).encode('utf-8'))
        producer.flush()

        print("produce kafka ==============================================================> success ")

        # 메세지 읽기
        spark = SparkSession.builder.appName('first').master("spark://"+spark_master_url).getOrCreate()

        print(spark)


        schema = StructType([
            StructField('id', LongType(), True),
            StructField('name', StringType(), True),
            StructField('age', StringType(), True)
            ])

        kafka_streaming_df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", bootstrap_servers) \
            .option("subscribe", "devices").load()

        # JSON 파싱
        json_df = kafka_streaming_df.selectExpr("CAST(value AS STRING) AS value")
        json_expanded_df = json_df.withColumn("value", from_json(json_df["value"], schema)).select("value.*")

        json_expanded_df.printSchema()

        print("read spark ==================================================================> success ")

    except Exception as e:
        print(f"오늘 안에 못끝낼걸 ?.. : {str}")

            # Avro로 변환하여 콘솔에 출력
        query = json_expanded_df.writeStream.trigger(processingTime='10 seconds') \
            .format("console").outputMode("complete").start()
        query.awaitTermination()

default_args = {
    'owner': 'admin',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
}

with DAG('check_connected', default_args=default_args, schedule=none) as dag:
    check_connected_task = PythonOperator(
        task_id='check_spark_airflow_connection',
        python_callable=check_connected,
        dag=dag,
    )
    messeaging_task = PythonOperator(
        task_id='messeaging',
        python_callable=messeaging,
        dag=dag,
        )

check_connected_task  messeaging_task


```

# 코드 메세지 확인

```bash
[appuser@broker ~]$ kafka-console-consumer --bootstrap-server broker:9092 --topic devices --from-beginning
{"id": 1, "name": "bami", "age": "7"}
{"id": 1, "name": "bami", "age": "7"}
```









```python
from pyspark.sql import SparkSession
from kafka import KafkaProducer
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError
from pyspark.sql.functions import from_json
from pyspark.sql.types import StringType, StructField, StructType, LongType
import json

# Kafka and Spark configurations
bootstrap_servers = '125.6.39.99:19092'
spark_master_url = '125.6.39.99:7077'
topic_name = 'devices'

# Absolute paths for data and checkpoint locations
data_path = '/opt/airflow/data'
checkpoint_location = '/opt/airflow/checkpoint'

# Function to check if Kafka is connected
def check_connected():
    try:
        admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)
        existing_topics = admin_client.list_topics()

        if topic_name in existing_topics:
            print(f"Topic '{topic_name}' already exists.")
        else:
            topic = NewTopic(name=topic_name, num_partitions=1, replication_factor=1)
            try:
                admin_client.create_topics([topic])
                print(f"Topic '{topic_name}' created successfully.")
            except TopicAlreadyExistsError:
                print(f"Topic '{topic_name}' already exists.")

        spark = SparkSession.builder.appName('first').master("spark://"+spark_master_url).getOrCreate()
        print("Spark is running:", spark)

        return spark

    except Exception as e:
        print(f"Error connecting to Kafka: {str(e)}")
        return None

# Function to send message to Kafka and process it with Spark
def messaging():

    spark = check_connected()
    if spark:
        try:
            # Send message to Kafka
            producer = KafkaProducer(bootstrap_servers=bootstrap_servers, linger_ms=20)
            message = {
                "id": 1,
                "name": "bami",
                "age": "7"
            }
            producer.send(topic_name, json.dumps(message).encode('utf-8'))
            producer.flush()
            print("Message sent to Kafka successfully.")

            # Define JSON schema
            schema = StructType([
                StructField('id', LongType(), True),
                StructField('name', StringType(), True),
                StructField('age', StringType(), True)
            ])
            print(schema) 
            
            # Read messages from Kafka and process with Spark
            kafka_streaming_df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", bootstrap_servers) \
                .option("subscribe", topic_name).load()

            json_df = kafka_streaming_df.selectExpr("CAST(value AS STRING) AS value","CAST(value AS STRING) AS value","CAST(value AS STRING) AS value")
            
            print(json_df)
            
            json_expanded_df = json_df.withColumn("value", from_json(json_df["value"], schema)).select("value.*")

            json_expanded_df.printSchema()
            print("Successfully read from Kafka and processed with Spark.")

            # Start Spark streaming query
            query = json_expanded_df.writeStream.trigger(processingTime='10 seconds') \
                .format("kafka").outputMode("append") \
                    .option("kafka.bootstrap.servers", bootstrap_servers) \
                    .option("topic", topic_name) \
                    .option("path", data_path) \
                    .option("checkpointLocation", checkpoint_location) \
                    .start()
            query.awaitTermination()

        except Exception as e:
            print(f"Error: {str(e)}")

# Run the messaging function
if __name__ == "__main__":
    messaging()
```

# 03-14


tasks.kafka_task
tasks.kafka_task import create_topic

