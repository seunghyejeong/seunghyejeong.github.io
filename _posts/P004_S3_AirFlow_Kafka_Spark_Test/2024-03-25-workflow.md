---

---
## Project Name: Spark Local 설치 
#### Date: 2024-03-25 09:45 
#### Tag:
---
# Contents:

- 클러스터 정보
```
baming
```

- IP 
```
125.6.40.10
```



https://spidyweb.tistory.com/303

```sh
curl -o kafka-clients-3.3.0.jar https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.3.0/kafka-clients-3.3.0.jar
curl -o spark-token-provider-kafka-0-10_2.12-3.3.0.jar https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar
curl -o spark-sql-kafka-0-10_2.12-3.3.0.jar https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar
curl -o commons-pool2-2.11.0.jar https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.0/commons-pool2-2.11.0.jar
curl -o spark-streaming-kafka-0-10_2.12-3.3.0.jar https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.12/3.3.0/spark-streaming-kafka-0-10_2.12-3.3.0.jar
```


```yaml
spark:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_MASTER_OPTS="-Dspark.rpc.message.maxSize=512"
spark-worker:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://{MASTER_IP}:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_WORKER_OPTS="-Dspark.rpc.message.maxSize=512"
```

## spark-defaults.conf

```
spark.master                     spark://master:7077
spark.eventLog.enabled           true
spark.driver.memory              5g
```
## spark-env.sh

```
- SPARK_MASTER_OPTS
```


## spark를 /home/ubuntu/.local 에 바로 해제 시키기 

- 실행시 
```
wget https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
```

> pyspark가 중복 되어 java runtime 에러가 나고, 
> 그래서 pip로 설치된 pyspark를 삭제하니 또 그거대로 모듈 import가 안되고, 
> spark 자체를 삭제하니 jar 파일을 못찾고,

- 이것 저것 시도 해도 자꾸 에러가 나길래 해당 airflow 폴더에 spark를 설치 해버렸다.
```
tar -xvf spark-3.3.0-bin-hadoop3.tgz --strip-components=1 -C /home/ubuntu/.local
```



```
[2024-03-25T13:33:57.126+0900] {scheduler_job_runner.py:1002} DEBUG - Next timed event is in 0.761329
[2024-03-25T13:33:57.126+0900] {scheduler_job_runner.py:1004} DEBUG - Ran scheduling loop in 0.14 seconds
[2024-03-25T13:33:57.889+0900] {scheduler_job_runner.py:983} DEBUG - Waiting for processors to finish since we're using sqlite
[2024-03-25T13:33:57.890+0900] {retries.py:93} DEBUG - Running DagWarning._purge_inactive_dag_warnings_with_retry with retries. Try 1 of 3


[2024-03-25T13:33:59.351+0900] {settings.py:386} DEBUG - Disposing DB connection pool (PID 8295)
[2024-03-25T13:33:59.404+0900] {processor.py:325} DEBUG - Waiting for <ForkProcess name='DagFileProcessor48-Process' pid=8295 parent=8243 stopped exitcode=0>

```

https://medium.com/@simardeep.oberoi/building-a-data-streaming-pipeline-leveraging-kafka-spark-airflow-and-docker-16527f9e9142
spark-submit을 그냥 실행하는데 ..? 

https://github.com/jeongseok912/airflow-spark-emr
여기서도 .. 

```
default_args = {
    'owner': 'admin',
    'depends_on_past': False,
    'start_date': dt(2024, 1, 1),
    'retries': 1,
    'provide_context': True,
}

with DAG('pipeline', default_args=default_args, schedule=None) as dag:
    kafka_processing = PythonOperator(
        task_id='kafka_streaming',
        python_callable=send_to_kafka,
        dag=dag,
    )
    spark_processing = PythonOperator(
        task_id='messaging',
        python_callable=messaging,
        dag=dag,
    )
```

```python
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError
import time
import uuid
import random
from datetime import datetime as dt
import json
from kafka import KafkaProducer
import requests
import os, sys

bootstrap_servers = '125.6.40.10:19092'
topic_name = 'devices'

def create_topic():
    try:
        admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)
        existing_topics = admin_client.list_topics()

        if topic_name in existing_topics:
            print(f"Topic '{topic_name}' already exists.")
        else:
            topic = NewTopic(name=topic_name)
            try:
                admin_client.create_topics([topic])
                print(f"Topic '{topic_name}' created successfully.")
                time.sleep(8)
            except TopicAlreadyExistsError:
                print(f"Topic '{topic_name}' already exists.")

    except Exception as e:
        print(f"Error connecting to Kafka: {str(e)}")

def devices(offset=0):

    device_id = [1, 2, 3, 4, 5]
    age_list = [23, 3, 7, 35, 37]
    name_list = ["jeong" , "ba", "mi" , "ji" , "won"]

    _event = {
        "eventId": str(uuid.uuid4()),
        "eventOffset": offset,
        "eventPublisher": "device",
        "data": {
            "devices": [
                {
                    "deviceId": random.choice(device_id),
                    "name": random.choice(name_list),
                    "age": random.choice(age_list)
                }
            ],
        },
        "eventTime": str(dt.now())
    }
    time.sleep(10)

    return json.dumps(_event).encode('utf-8')

def kafka_streaming():
    _offset = 100
    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)
    try:
        while True:
            data = devices(offset=_offset)
            producer.send('devices', key=b'device', value=data)
            producer.flush()
            print("Posted to topic")
            time.sleep(random.randint(0, 5))
            _offset += 1
    except Exception as e:
        print(f"Error: {e}")
    finally:
        producer.close()

if __name__ == "__main__":
    kafka_streaming()
```

query = parsed_df \
.writeStream \
.format("json") \
        .option("checkpointLocation", "checkpoint_dir") \
        .option("path", /opt/bitnami/spark/data) \
        .outputMode("append") \
        .start()

- spark_processing
```python
import os
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.functions import col, pandas_udf, split

bootstrap_servers = '125.6.40.10:19092'
topic_name = 'devices'
spark_version = '3.3.0'
os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12-3.3.0,org.apache.spark:spark-streaming-kafka-0-10_2.12-3.3.0,org.apache.spark:commons-pool2-2.11.0,org.apache.spark:kafka-clients-3.3.0'.format(spark_version)
spark_master_url = 'local[*]'

spark = SparkSession \
        .builder \
        .appName("PipelineApp")\
        .master("local[*]")\
        .config('spark.jars.package', 'org.apache.spark:spark-sql-kafka-0-10_2.12-3.3.0.jar')\
        .config("spark.sql.execution.arrow.pyspark.enabled", "true")\
        .getOrCreate()

spark = SparkSession.builder.getOrCreate()


df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", bootstrap_servers).option("failOnDataLoss","False").option("subscribe",topic_name).load()

df.isStreaming
df.printSchema()

schema = StructType([
    StructField("eventId", StringType(), True),
    StructField("eventOffset", StringType(), True),
    StructField("eventPublisher", StringType(), True),
    StructField("data", StructType([
        StructField("devices", ArrayType(StructType([
            StructField("deviceId", StringType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])), True)
    ]), True),
    StructField("eventTime", StringType(), True)
])

jsonSchema = schema

parsed_df = df.selectExpr("CAST(value AS STRING) as json").select(from_json("json", jsonSchema).alias("data")).select("data.*")

query = parsed_df.writeStream.format("console").start()

query = parsed_df.writeStream \
        .format("json") \
        .option("checkpointLocation", "checkpoint_dir") \
        .option("path", "/opt/bitnami/spark/data") \
        .outputMode("append") \
        .start()

query.awaitTermination()

def get_token(auth_url, tenant_id, username, password):
    token_url = auth_url + '/tokens'
    req_header = {'Content-Type': 'application/json'}
    req_body = {
        'auth': {
            'tenantId': tenant_id,
            'passwordCredentials': {
                'username': username,
                'password': password
            }
        }
    }

    response = requests.post(token_url, headers=req_header, json=req_body)
    return response.json()

class ObjectService:
    def __init__(self, storage_url, token_id):
        self.storage_url = storage_url
        self.token_id = token_id

    def _get_url(self, container, object):
        return '/'.join([self.storage_url, container, object])

    def _get_request_header(self):
        return {'X-Auth-Token': self.token_id}

    def upload(self, container, object, object_path):
        req_url = self._get_url(container, object)
        req_header = self._get_request_header()

        path = '/'.join([object_path, object])
        with open(path, 'rb') as f:
            return requests.put(req_url, headers=req_header, data=f.read())

def send_to_storage():

    AUTH_URL = 'https://api-identity-infrastructure.nhncloudservice.com/v2.0'
    TENANT_ID = '9ea3a098cb8e49468ac2332533065184'
    USERNAME = 'minkyu.lee'
    PASSWORD = 'K-PaaS@2023'

    token = get_token(AUTH_URL, TENANT_ID, USERNAME, PASSWORD)
    print(json.dumps(token, indent=4))

    STORAGE_URL = 'https://kr1-api-object-storage.nhncloudservice.com/v1/AUTH_9ea3a098cb8e49468ac2332533065184'
    TOKEN_ID = 'gAAAAABl751bvzYtIVNp-JkDmj2XvF1QLGzSJ08kXAdMgqnMoHEm8hAlqrUieTYqMJcsjpMVGUF91rSeqGo7-VyjDnDBqwzlMwOLzXynIT0xYmAPeyHNA6eKhuZDi7QxEqutYjGA5ddjbSonaQmg3ZTk7YwitvoWyMIc8hItJgZXY2kdRF2HyZs'
    CONTAINER_NAME = 'cp-object-storage'
    OBJECT_NAME = '*.json'
    OBJECT_PATH = r'/opt/bitnami/spark/data'

    
    obj_service = ObjectService(STORAGE_URL, TOKEN_ID)
    obj_service.upload(CONTAINER_NAME, OBJECT_NAME, OBJECT_PATH)

if __name__ == "__main__":
    send_to_storage()
```

kafka_streaming
kafka_streaming.py
kafka_streaming
kafka_streaming:q

```
def get_token(auth_url, tenant_id, username, password):
    token_url = auth_url + '/tokens'
    req_header = {'Content-Type': 'application/json'}
    req_body = {
        'auth': {
            'tenantId': tenant_id,
            'passwordCredentials': {
                'username': username,
                'password': password
            }
        }
    }

    response = requests.post(token_url, headers=req_header, json=req_body)
    return response.json()

class ObjectService:
    def __init__(self, storage_url, token_id):
        self.storage_url = storage_url
        self.token_id = token_id

    def _get_url(self, container, object):
        return '/'.join([self.storage_url, container, object])

    def _get_request_header(self):
        return {'X-Auth-Token': self.token_id}

    def upload(self, container, object, object_path):
        req_url = self._get_url(container, object)
        req_header = self._get_request_header()

        path = '/'.join([object_path, object])
        with open(path, 'rb') as f:
            return requests.put(req_url, headers=req_header, data=f.read())

def send_to_storage():

    AUTH_URL = 'https:ce.com/v2.0'
    TENANT_ID = '9e4'
    USERNAME = 'miee'
    PASSWORD = 'K-P3'

    token = get_token(AUTH_URL, TENANT_ID, USERNAME, PASSWORD)
    print(json.dumps(token, indent=4))

    STORAGE_URL = 'https://kr1-api-object-storage.nhncloudservice.com/v1/AUTH_9ea84'
    TOKEN_ID = 'gAzlM+()*_(*)_*$%^7543Tk7YwZs'
    CONTAINER_NAME = 'ce'
    OBJECT_NAME = '*.json'
    OBJECT_PATH = r'/opt/bitnami/spark/data'


    obj_service = ObjectService(STORAGE_URL, TOKEN_ID)
    obj_service.upload(CONTAINER_NAME, OBJECT_NAME, OBJECT_PATH)

if __name__ == "__main__":
    send_to_storage()

```

kafka-python
pandas==1.3.5
requests==2.31.0
selenium==4.17.2
beautifulsoup4==4.12.3
lxml==5.1.0



```java
logger.kafkaConsumer.name = org.apache.kafka.clients.consumer
logger.kafkaConsumer.level = warn
```
spark-submit --master local[*] --name PipelineApp --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 --name PipelineApp ./msg.py 



 NHN / 인스턴스 1대 / spark 테스트
 
 ---


*1. 클라우드* : NHN
*2. 서비스명* : 
|_.인스턴스|_.호스트명|
|  | baming |

*2. 연장날짜* : 03/25 ~ 03/27
*3. 연장사유* : 
* spark-submit 테스트 환경 구성이 되어있음.