---

---
## Project Name: airflow에서 spark 연결  
#### Date: 2024-03-22 09:01 
#### Tag:
---
# Contents:

- [b] REF
>

```
133.186.212.113
```


# 계속 뜨는 spark-submit error
```
Error: Missing application resource.

Usage: spark-submit [options] <app jar | python file | R file> [app arguments]
Usage: spark-submit --kill [submission ID] --master [spark://...]
Usage: spark-submit --status [submission ID] --master [spark://...]
Usage: spark-submit run-example [options] example-class [example args]

Options:
```
: 이건 내가 pyspark를 pip install 로 설치한 바람에 아무 쓰임도 없어도 될 spark-submit 명령어가 쓰이고 있었음.
`pip uninstall pyspark`를 하니까 명령어가 사용도 안되고 정상적으로 Dag이 import 되었음.

```
spark-submit --master spark://133.186.212.113:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 --name PipelineApp msg.py --deploy-mode cluster
```

```
spark-submit --master local[*] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 --name PipelineApp ./msg.py
```

# operator
1. spark-submit을 이용하기 위해 Airflow 서버로 띄웠을 경우 그 환경에는 Spark가 설치 되어 있어야 한다.
> [[2024-03-15-workflow]] : SparkSubmitOperator는 spark-submit 명령어를 실행하기에 필요한 spark 소프트웨어를 모두 설치해 주어야 한다.

2. 그래서 생각한 Docker container에 접속 할 방법을 찾던 중 .. 
> ssh operator?
> docker operator?
> [dockerOperator Official Docs](https://airflow.apache.org/docs/apache-airflow-providers-docker/2.7.0/_api/airflow/providers/docker/operators/docker/index.html)

#### sshOperator는 의미가 없는 것 같고,
```
templated_bash_command = """
    spark-submit \
    --class my_class \
    --master spark://spark-master:7077 \
    --executor-cores 2 \
    --executor-memory 2g \
    my_first.jar
"""
hook = SSHHook(
    ssh_conn_id='ssh_default',
    remote_host='spark-master',
    username='username',
    key_file='~/.ssh/id_rsa'
)
run_ssh = SSHOperator(
    task_id='spark_submit_task',
    ssh_hook=hook,
    command=templated_bash_command,
    dag=dag
)
```

#### Dag을 실행 할 때 DockerOperator로 docker container를 띄울 수 있다.
> 그런데 굳이.. container로 띄우는게 의미가 있나 ? 싶음.

1. dockerOperator를 발견 했는데 이는 docker container를 띄워 job을 실행하는 operator이다 
```
spark_task = DockerOperator(
    task_id='run_spark_task',
    image='your/spark_image:tag',
    command=spark_command,
    api_version='auto',
    auto_remove=True,
    dag=dag,
)
```
`auto_remove`를 통해 job을 실행 한 후 컨테이너를 유지 시킬 것인가 아닌가를 설정 할 수 있다. 

```python
    spark_master_task = DockerOperator(
        task_id='run_spark_master',
        image='seunghyejeong/spark:1.0',
        environment={
            'SPARK_MODE': 'master',
            'SPARK_RPC_AUTHENTICATION_ENABLED': 'no',
            'SPARK_RPC_ENCRYPTION_ENABLED': 'no',
            'SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED': 'no',
            'SPARK_SSL_ENABLED': 'no',
            'SPARK_USER': 'spark',
            'SPARK_MASTER_OPTS': '-Dspark.rpc.message.maxSize=512'
        },
        ports=[8090, 7077],  # 포트 매핑
        network_mode='bridge',  # 네트워크 설정
        auto_remove=True,  # 컨테이너 자동 제거
        dag=dag,
    )
    
    # Spark Worker 실행을 위한 DockerOperator 설정
    spark_worker_task = DockerOperator(
        task_id='run_spark_worker',
        image='seunghyejeong/spark:1.0',
        environment={
            'SPARK_MODE': 'worker',
            'SPARK_MASTER_URL': 'spark://133.186.212.113:7077',
            'SPARK_WORKER_MEMORY': '1G',
            'SPARK_WORKER_CORES': '1',
            'SPARK_RPC_AUTHENTICATION_ENABLED': 'no',
            'SPARK_RPC_ENCRYPTION_ENABLED': 'no',
            'SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED': 'no',
            'SPARK_SSL_ENABLED': 'no',
            'SPARK_USER': 'spark',
            'SPARK_WORKER_OPTS': '-Dspark.rpc.message.maxSize=512'
        },
        network_mode='bridge',  # 네트워크 설정
        auto_remove=True,  # 컨테이너 자동 제거
        dag=dag,
    )
    # DAG에 작업 추가
    spark_master_task >> spark_worker_task
```


{"eventId": "3448694a-56b1-43c0-a93e-99c1915dd910", "eventOffset": 103, "eventPublisher": "device", "data": {"devices": [{"deviceId": 4, "name": "ji", "age": 35}]}, "eventTime": "2024-03-22 10:10:51.777997"}


kafka-topics --describe --topic devices --bootstrap-server broker:9092

kafka-console-producer --topic devices --bootstrap-server broker:19092

kafka-console-consumer --bootstrap-server broker:9092 --topic devices --from-beginning


```python
spark-submit --master local[*] \
--name PipelineApp \
--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
--name PipelineApp \
./msg.py 
```

.writeStream \
.format("parquet") \
.outputMode("append") \
.option("checkpointLocation", "/check") \
.option("path", "/test") \
.start()

```python
import os
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.functions import col, pandas_udf, split

bootstrap_servers = '125.6.40.10:19092'
topic_name = 'devices'
spark_version = '3.3.0'
os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12-3.3.0,org.apache.spark:spark-streaming-kafka-0-10_2.12-3.3.0,org.apache.spark:commons-pool2-2.11.0,org.apache.spark:kafka-clients-3.3.0'.format(spark_version)
spark_master_url = '125.6.40.10:7077'

spark = SparkSession \
        .builder \
        .appName("PipelineApp")\
        .master("spark://"+spark_master_url)\
        .config('spark.jars', '$SPARK_HOME/jars/spark-sql-kafka-0-10_2.12-3.3.0.jar')\
        .config("spark.sql.execution.arrow.pyspark.enabled", "true")\
        .getOrCreate()


df = spark.readStream.format("kafka").option("kafka.bootstrap.servers",bootstrap_servers).option("failOnDataLoss","False").option("subscribe",topic_name).load()


df.isStreaming
df.printSchema()

schema = StructType([
    StructField("eventId", StringType(), True),
    StructField("eventOffset", StringType(), True),
    StructField("eventPublisher", StringType(), True),
    StructField("data", StructType([
        StructField("devices", ArrayType(StructType([
            StructField("deviceId", StringType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])), True)
    ]), True),
    StructField("eventTime", StringType(), True)
])

jsonSchema = schema

parsed_df = df.selectExpr("CAST(value AS STRING) as json").select(from_json("json", jsonSchema).alias("data")).select("data.*")

query = parsed_df \
.writeStream \
.format("json") \
.option("checkpointLocation", "checkpoint_dir") \
.option("path", "result") \
.outputMode("append") \
.start()

query.awaitTermination()
```


 docker run --name postgres -e POSTGRES_USER=airflow -e POSTGRES_PASSWORD=1234 -p 5432:5432 postgres:13

postgres=# create airflow testhyun password '1234' superuser;



[postgresql 설정](https://taufiq-ibrahim.medium.com/apache-airflow-installation-on-ubuntu-ddc087482c14)




---
# Dockerfile
```
FROM bitnami/spark:3.3.0

USER root
COPY msg.py /opt/bitnami/spark


# Install necessary packages
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        vim \
        curl \
        netcat-openbsd && \
    rm -rf /var/lib/apt/lists/*

# Download Kafka client JAR
RUN curl -o /opt/bitnami/spark/jars/kafka-clients-3.3.0.jar https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.3.0/kafka-clients-3.3.0.jar

RUN curl -o /opt/bitnami/spark/jars/spark-streaming-kafka-0-10_2.12-3.3.0 https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.12/3.3.0/spark-streaming-kafka-0-10_2.12-3.3.0.jar

# Download Spark Kafka connector JAR
RUN curl -o /opt/bitnami/spark/jars/spark-token-provider-kafka-0-10_2.12-3.3.0.jar https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar

# Download Spark SQL Kafka connector JAR
RUN curl -o /opt/bitnami/spark/jars/spark-sql-kafka-0-10_2.12-3.3.0.jar https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar

RUN curl -o /opt/bitnami/spark/jars/commons-pool2-2.11.0.jar https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.0/commons-pool2-2.11.0.jar

RUN pip install py4j==0.10.9.5

USER 1001

```

