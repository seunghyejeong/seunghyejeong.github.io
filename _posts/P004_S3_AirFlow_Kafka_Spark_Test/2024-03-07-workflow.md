```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json
from pyspark.sql.types import StructType, StringType

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("KafkaToDataFile") \
    .getOrCreate()

# Define Kafka parameters
kafka_params = {
    "kafka.bootstrap.servers": "133.186.134.16:19092",
    "subscribe": "funnytopic"
}
    
    # Define schema for the incoming Kafka message
    schema = StructType().add("key", StringType()).add("value", StringType())
    
    # Define the output data file path
    output_path = "/opt/airflow/datas"
    
    def consume_and_export():
    # Create DataFrame representing the stream of input lines from Kafka
    kafka_df = spark.readStream \
        .format("kafka") \
        .options(**kafka_params) \
        .load()

    # Parse the value column from Kafka as JSON
    parsed_df = kafka_df.selectExpr("CAST(value AS STRING)") \
        .select(from_json("value", schema).alias("data")) \
        .select("data.*")

    # Start the query to continuously write the parsed DataFrame to a data file
    query = parsed_df.writeStream \
        .format("csv") \
        .option("path", output_path) \
        .option("checkpointLocation", "/opt/airflow/offset/a") \
        .outputMode("append") \
        .start()

    # Await termination
    query.awaitTermination()


if __name__ == "__main__":
    consume_and_export()

```