---

---
## Project Name: objectstorage 저장 
#### Date: 2024-03-28 09:59 
#### Tag:
---
# Contents:

- [i] IP 정보 

- 노드 이름 
```

```

- IP
```
125.6.40.10
```

### hdfs 저장소에 저장하기


- [b] REF
> [spark openstack swift API](https://spark.apache.org/docs/2.2.0/storage-openstack-swift.html)

- apache hadoop openstack
spark-hadoop-cloud_2.13-3.3.0.jar https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.13/3.3.0/spark-hadoop-cloud_2.13-3.3.0.jar

hadoop-openstack-3.3.0.jar https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-openstack/3.3.0/hadoop-openstack-3.3.0.jar

| Property Name                                    | Meaning                          | Required  |
| ------------------------------------------------ | -------------------------------- | --------- |
| `fs.swift.service.PROVIDER.auth.url`             | Keystone Authentication URL      | Mandatory |
| `fs.swift.service.PROVIDER.auth.endpoint.prefix` | Keystone endpoints prefix        | Optional  |
| `fs.swift.service.PROVIDER.tenant`               | Tenant                           | Mandatory |
| `fs.swift.service.PROVIDER.username`             | Username                         | Mandatory |
| `fs.swift.service.PROVIDER.password`             | Password                         | Mandatory |
| `fs.swift.service.PROVIDER.http.port`            | HTTP port                        | Mandatory |
| `fs.swift.service.PROVIDER.region`               | Keystone region                  | Mandatory |
| `fs.swift.service.PROVIDER.public`               | Indicates if all URLs are public | Mandatory |

org.apache.spark:spark-hadoop-cloud_2.13-3.3.0.jar

fs.swift.service.openstack.auth.url = 'https://api-identity-infrastructure.nhncloudservice.com/v2.0'
fs.swift.service.openstack.tenant = '9ea3a098cb8e49468ac2332533065184'
fs.swift.service.openstack.username = 'minkyu.lee'
fs.swift.service.openstack.password = 'PaaS-TA@2024!'
fs.swift.service.openstack.region = 'KR1'
fs.swift.service.openstack.apikey = ''



PasswordCredentials

org.apache.hadoop.fs.swift.auth.getApiKey()
setApi

### 체크 리스트
> 완벽하게 아주 연결 된 것은 아니지만 object storage로 올리기 까지 성공 했 다..  코드 마무리 해야 할 부분이 있음
- [?] 왜 output 경로가 달라져야 정상 작동 하는지 

- 하면 좋은 것들  
- [ ] spark command는 bash operator로 해결하기 
- [ ] spark messging 파일에 Object Storage로 보내는 코드 합치기.
        1. 스토리지로 바로 보내기 ?
- [x] object storage에 올릴 때 파일 이름 넣는 코드 (파일 이름이 완벽히 일치해야 업로드 가능)
- [x] token값은 계속 달라짐 
    - 토큰 값을 받아와 소스에 넣고 실행 되게 하는 코드 

![[Pasted image 20240328173722.png]]
![[Pasted image 20240328173757.png]]



## 🤔 Hadoop을 왜 쓰는지 알겠네..

스파크가 Master와 Worker 등 여러 곳에 분산 처리하는 시스템이기 때문에 데이터가 worker의 어딘 가에 저장된다. Master가 여러 대 구성 되었을 경우에  Master가 죽어도 시스템은 계속 돌아가는 시스템.
그래서 데이터는 어느 한 DB에 모여야 한 곳에 저장이 되고 , 그래야  스토리지로 데이터를 보낼 수 있음 
그래서 
1. 스토리지로 바로 보내는 코드를 작성한다.
2. Hadoop과 연결한다 

hadoop download
```sh
curl -o hadoop-3.3.2 https://archive.apache.org/dist/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz
```

환경 변수 설정
```bash
export $HADOOP_HOME = /opt/bitnami/hadoop
export $HADOOP_CONFIG_HOME = $HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
```

id hadoop/1234
