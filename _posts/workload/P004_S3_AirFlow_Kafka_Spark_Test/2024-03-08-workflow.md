---

---
## Project Name:  Code Test (Kafka-spakr at airflow )
#### Date: 2024-03-08 10:10 
#### Tag:
---
# Contents:

- [b] REF
>

1
```
180.210.82.60
```
2
```
180.210.82.237
```


```python
from airflow import DAG
from pyspark.sql import SparkSession
from airflow.operators.python_operator import PythonOperator
from kafka import KafkaProducer, KafkaConsumer
from kafka.admin import KafkaAdminClient, NewTopic
from datetime import datetime
from airflow.models import TaskInstance
from kafka.errors import TopicAlreadyExistsError


bootstrap_servers = '180.210.82.237:19092'
spark_master_url = '180.210.82.237:7077'
topic_name = 'devices'

# Function to check if Kafka is connected
def check_connected():
    try:
        producer = KafkaProducer(bootstrap_servers=bootstrap_servers)
        consumer = SparkSession.builder.getOrCreate()
        
        print(consumer)

        spark_context = spark_session.sparkContext
        spark_version = spark_sesion.version

        print("Successfully connected to Spark & Kafk!")
        return True

    except Exception as e:
        print(f"Error connecting to Kafka: {str(e)}")
        return False
        

def create_topic():
    admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)
    existing_topics = admin_client.list_topics()
    
    if topic_name in existing_topics:
        print(f"Topic '{topic_name}' already exists.")
    else:
        topic = NewTopic(name=topic_name, num_partitions=1, replication_factor=1)
        try:
            admin_client.create_topics([topic])
            print(f"Topic '{topic_name}' created successfully.")
        except TopicAlreadyExistsError:
            print(f"Topic '{topic_name}' already exists.")

# Function to produce a message to Kafka
def produce_message():
    producer = KafkaProducer(bootstrap_servers=bootstrap_servers,linger_ms=20)
    message = f"Hello from Airflow. Message produced at {datetime.now()}"
    producer.send(topic_name, message.encode())
    producer.flush()

def consume_message():
    spark = SparkSession.getActiveSession()
    df = sprk.readStream.format("kafka") \ 
    .option("kafka.bootstrap.servers", "host1:port1, host2:port2") \
    .option("subscribe", "topic1") \
    .load()
    
    val ds = spark.writeStream.trigger(processingTime='10 seconds') \
        .format("console").outputMode("complete") \
        .start()
```

```python
val stream = 
spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", bootstrap_servers)
    .option("subscribe", "topic1")
    .load()
    .groupBy(col("value").cast("string").as("key"))
    .agg("count(*).as("value"))
    .writeStream
    .format("kafka")
    .option("output-wordcount")
    .trigger("1 minute")
    .outputMode("update")
    .option("checkpointLocation", "s3://...")
    .start()

stream.awaitTermination()
```

```python
def consume_message():
    spark = SparkSession.getActiveSession()
    
    df = sprk.readStream.format("kafka") \ 
    .option("kafka.bootstrap.servers", "host1:port1, host2:port2") \
    .option("subscribe", "topic1") \
    .load()

    schema = df.

```