---

---
## Project Name:  airflow Config 설정
#### Date: 2024-03-21 09:39 
#### Tag:
---
# Contents:

- [b] REF
> https://velog.io/@bbkyoo/AirFlow-%EC%84%A4%EC%B9%98Celery-Cluster
> [airflow 설정 개념 설명](https://velog.io/@bbkyoo/AirFlow-%EC%84%A4%EC%B9%98Celery-Cluster)

[[Airflow 활용 Guide]] : 수정중
    + java 설치
    + pyspark 버전에 맞게 설치
    + 각 HOME dir 환경 변수 처리 해주기 

- 병렬 처리를 위한 설정 
![[Pasted image 20240321093940.png]]
```
133.186.222.229
```

```
environ({'SHELL': '/bin/bash', 'PYTHONHASHSEED': '0', 'PYSPARK_DRIVER_PYTHON': 'python3', 'JAVA_HOME': '/usr/lib/jvm/java-8-openjdk-amd64', 'PWD': '/home/ubuntu/airflow', 'LOGNAME': 'ubuntu', 'XDG_SESSION_TYPE': 'tty', 'MOTD_SHOWN': 'pam', 'HOME': '/home/ubuntu', 'LANG': 'C.UTF-8', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'VIRTUAL_ENV': '/home/ubuntu/airflow/env', 'AIRFLOW_HOME': '/home/ubuntu/.local', 'PYTHONSTARTUP': '/home/ubuntu/airflow/env/lib/python3.10/site-packages/pyspark/python/pyspark/shell.py', 'SSH_CONNECTION': '218.51.176.155 63814 172.16.11.46 22', 'PYSPARK_PYTHON': 'python3', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'XDG_SESSION_CLASS': 'user', 'PYTHONPATH': '/home/ubuntu/airflow/env/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip:/home/ubuntu/airflow/env/lib/python3.10/site-packages/pyspark/python/:', 'TERM': 'xterm', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'USER': 'ubuntu', 'OLD_PYTHONSTARTUP': '', 'DISPLAY': 'localhost:12.0', 'SHLVL': '1', 'SPARK_HOME': '/home/ubuntu/airflow/env/lib/python3.10/site-packages/pyspark', 'XDG_SESSION_ID': '16', 'VIRTUAL_ENV_PROMPT': 'env', 'SPARK_CONF_DIR': '/home/ubuntu/airflow/env/lib/python3.10/site-packages/pyspark/conf', 'XDG_RUNTIME_DIR': '/run/user/1000', 'SSH_CLIENT': '218.51.176.155 63814 22', '_SPARK_CMD_USAGE': 'Usage: ./bin/pyspark [options]', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SPARK_ENV_LOADED': '1', 'PATH': '/home/ubuntu/airflow/env/bin:/home/ubuntu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus', 'SPARK_SCALA_VERSION': '2.12', 'SSH_TTY': '/dev/pts/6', 'OLDPWD': '/home/ubuntu', 'PYSPARK_SUBMIT_ARGS': '"--name" "PySparkShell" "pyspark-shell"', 'SPARK_AUTH_SOCKET_TIMEOUT': '15', 'SPARK_BUFFER_SIZE': '65536'})

```


export PYSPARK_SUBMIT_ARGS="--master spark://133.186.217.128:7077"


export AIRFLOW_CONN_SPARK_DEFAULT='spark://133.186.222.229:7077?deploy-mode=cluster&spark_binary=command'


```
    spark_job = SparkSubmitOperator(
        task_id='spark_job',
        conn_id='spark_default',
        application='/opt/bitnami/spark/jars',
        java_class='org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0'
        conf={
            'master': 'local[*]',
            'spark.driver.memory': '2g',
            'spark.executor.memory': '2g',
            'spark.default.parallelism': '16',
            'spark.local.dir': '/opt/bitnami/spark',
            'spark.driver.cores': 4,
            'spark.executor.cores': 3
        },
    )
```


# jar 파일이 있는 곳에서 spark-submit을 실행해야함 

### spark-submit --master spark://133.186.222.229:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 --name PipelineApp msg.py --deploy-mode cluster

### sspark-submit --master spark://133.186.212.113:7077 --jars spark-sql-kafka-0-10_2.12-3.3.0.jar,spark-streaming-kafka-0-10_2.12-3.3.0.jar --name PipelineApp msg.py

```
    dspark_submit_task = SparkSubmitOperator(
        task_id='spark_submit_task',
        conn_id='spark_default', 
        application="msg.py",
        total_executor_cores='4',
        executor_cores='2',
        executor_memory='2g',
        num_executors='2',
        name='PipelineApp',
        packages= ['org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0', 'commons-pool2-2.11.0.jar'],
        master='spark://133.186.222.229:7077',
        dag=dag,
    )

```

```
with DAG(
        dag_id='spark-demo-dag',
        default_args=default_args,
        schedule_interval=None
) as dag:
    podRun = SparkSubmitOperator(
        task_id='demo-spark-app-id',
        conn_id='spark_default',
        application='local:///opt/spark/jars/spark-on-eks-example-assembly-v1.0.jar',
        name='spark-on-eks-example',
        java_class='ExampleApp',
        conf={
            'spark.kubernetes.container.image': '{}.dkr.ecr.{}.amazonaws.com/test/spark:spark-on-eks'.format(aws_account_id, aws_region),
            'spark.hadoop.fs.s3a.impl': 'org.apache.hadoop.fs.s3a.S3AFileSystem',
            'spark.kubernetes.container.image.pullPolicy': 'Always',
            'spark.kubernetes.authenticate.driver.serviceAccountName': '{}'.format(k8s_service_account_name),
            'spark.kubernetes.authenticate.executor.serviceAccountName': '{}'.format(k8s_service_account_name),
            'spark.kubernetes.namespace': '{}'.format(k8s_namespace),
            'spark.kubernetes.node.selector.topology.kubernetes.io/zone': '{}a'.format(aws_region),
            'spark.hadoop.fs.s3a.aws.credentials.provider': 'com.amazonaws.auth.WebIdentityTokenCredentialsProvider',
            'spark.kubernetes.authenticate.submission.caCertFile': k8s_ca_crt_path,
            'spark.kubernetes.authenticate.submission.oauthTokenFile': k8s_sa_token_path
            # 'spark.kubernetes.authenticate.submission.oauthToken': secret_token
        },
        verbose=True,
        application_args=['s3a://test-bucket/input.csv', 's3a://test-bucket/result/spark'],
        env_vars={
            'KUBECONFIG': kube_config_path
        }
    )
```




/opt/bitnami/spark/jarsspark-sql-kafka-0-10_2.12-3.3.0.jar


airflow venv가 ..제일마지막에 설정되어야 할거같다
# jar

```bash
curl -o kafka-clients-3.3.0.jar https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.3.0/kafka-clients-3.3.0.jar
curl -o spark-token-provider-kafka-0-10_2.12-3.3.0.jar https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar
curl -o spark-sql-kafka-0-10_2.12-3.3.0.jar https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar
curl -o commons-pool2-2.11.0.jar https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.0/commons-pool2-2.11.0.jar
curl -o spark-streaming-kafka-0-10_2.12-3.3.0.jar https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.12/3.3.0/spark-streaming-kafka-0-10_2.12-3.3.0.jar
```

```
ubuntu@baming:~/airflow$ cat requirements.txt
pandas==1.3.5
requests==2.31.0
selenium==4.17.2
beautifulsoup4==4.12.3
lxml==5.1.0
virtualenv
kafka-python==2.0.2
apache-airflow-providers-apache-kafka==1.3.1
confluent-kafka==2.3.0
apache-airflow-providers-apache-spark==2.1.0
py4j==0.10.9.5
grpcio-status>=1.59.0
pyspark==3.3.0
```

```
ubuntu@baming:~/airflow$ cat install.sh
# Airflow needs a home. `~/airflow` is the default, but you can put it
# somewhere else if you prefer (optional)
export AIRFLOW_HOME=~/airflow
export PATH=$PATH:/home/ubuntu/.local/bin
# Install Airflow using the constraints file
AIRFLOW_VERSION=2.3.4
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
# For example: 3.7
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
# For example: https://raw.githubusercontent.com/apache/airflow/constraints-2.3.4/constraints-3.7.txt
pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
```


```python
from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime as dt
import msg
from topic import create_topic
from send import send_to_kafka
import time
from time import sleep
from datetime import timedelta

default_args = {
    'owner': 'admin',
    'depends_on_past': False,
    'start_date': dt(2024, 1, 1),
    'retries': 1,
    'provide_context': True,
    'retry_delay': timedelta(minutes=1)
}

with DAG('pipeline', default_args=default_args, schedule=None) as dag:

    create_topic_task = PythonOperator(
        task_id='create_topic',
        python_callable=create_topic,
        dag=dag,
    )
    send_to_kafka_task = PythonOperator(
        task_id='send_to_kafka',
        python_callable=send_to_kafka,
        dag=dag,
    )
    messaging_task = PythonOperator(
        task_id='messaging',
        python_callable=messaging,
        dag=dag,
    )
    spark_submit_task = SparkSubmitOperator(
        task_id='spark_submit_task',
        conn_id='spark_default',
        application="msg.py",
        name='PipelineApp',
        packages='org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0',
        deploy_mode='cluster',
        dag=dag,
    )

```

msg 
```python
import os
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.functions import col, pandas_udf, split

bootstrap_servers = '125.6.40.10:19092'
topic_name = 'devices'
spark_version = '3.3.0'
os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12-3.3.0,org.apache.spark:spark-streaming-kafka-0-10_2.12-3.3.0,org.apache.spark:commons-pool2-2.11.0,org.apache.spark:kafka-clients-3.3.0'.format(spark_version)
spark_master_url = 'local[*]'

spark = SparkSession \
        .builder \
        .appName("PipelineApp")\
        .master("local[*]")\
        .config('spark.jars.package', 'org.apache.spark:spark-sql-kafka-0-10_2.12-3.3.0.jar')\
        .config("spark.sql.execution.arrow.pyspark.enabled", "true")\
        .getOrCreate()


df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", bootstrap_servers).option("failOnDataLoss","False").option("subscribe",topic_name).load()

df.isStreaming
df.printSchema()

schema = StructType([
    StructField("eventId", StringType(), True),
    StructField("eventOffset", StringType(), True),
    StructField("eventPublisher", StringType(), True),
    StructField("data", StructType([
        StructField("devices", ArrayType(StructType([
            StructField("deviceId", StringType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])), True)
    ]), True),
    StructField("eventTime", StringType(), True)
])

jsonSchema = schema

parsed_df = df.selectExpr("CAST(value AS STRING) as json").select(from_json("json", jsonSchema).alias("data")).select("data.*")

query = parsed_df.writeStream.format("console").start()

query.awaitTermination()
```

topic
```python
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError
import time

bootstrap_servers = '125.6.40.10:19092'
topic_name = 'devices'

def create_topic():
    try:
        admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)
        existing_topics = admin_client.list_topics()

        if topic_name in existing_topics:
            print(f"Topic '{topic_name}' already exists.")
        else:
            topic = NewTopic(name=topic_name)
            try:
                admin_client.create_topics([topic])
                print(f"Topic '{topic_name}' created successfully.")
                time.sleep(8)
            except TopicAlreadyExistsError:
                print(f"Topic '{topic_name}' already exists.")

    except Exception as e:
        print(f"Error connecting to Kafka: {str(e)}")

```

send 
```python
import uuid
import random
from datetime import datetime as dt
import time
import json
from kafka import KafkaProducer

bootstrap_servers = '133.186.152.164:19092'
topic_name = 'devices'

def devices(offset=0):

    device_id = [1, 2, 3, 4, 5]
    age_list = [23, 3, 7, 35, 37]
    name_list = ["jeong" , "ba", "mi" , "ji" , "won"]

    _event = {
        "eventId": str(uuid.uuid4()),
        "eventOffset": offset,
        "eventPublisher": "device",
        "data": {
            "devices": [
                {
                    "deviceId": random.choice(device_id),
                    "name": random.choice(name_list),
                    "age": random.choice(age_list)
                }
            ],
        },
        "eventTime": str(dt.now())
    }
    time.sleep(10)

    return json.dumps(_event).encode('utf-8')

def send_to_kafka():
    _offset = 100
    producer = KafkaProducer(bootstrap_servers=bootstrap_servers)
    try:
        while True:
            data = devices(offset=_offset)
            producer.send('devices', key=b'device', value=data)
            producer.flush()
            print("Posted to topic")
            time.sleep(random.randint(0, 5))
            _offset += 1
    except Exception as e:
        print(f"Error: {e}")
    finally:
        producer.close()
```