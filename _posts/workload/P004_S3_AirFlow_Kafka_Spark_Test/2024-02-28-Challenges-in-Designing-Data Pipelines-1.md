---
title: The long adventure to designing Pipeline...1) Edit & Analyst DataProccess Source
author: bami jeong
categories: build
layout: post
comments: true
tags:
  - DataPipeline
  - Spark
  - Airflow
  - Docker
  - Kafka
---

> [!warning] 
> 
> ğŸ«  ë§¤ìš° ê¸´ ì±Œë¦°ì§€ê°€ ì‹œì‘ë©ë‹ˆë‹¤.
> A Lengthy Challenge Begins...



- [b] REF
> [ì˜ˆì œ ì½”ë“œ](https://medium.com/swlh/using-airflow-to-schedule-spark-jobs-811becf3a960)
> [postgresì— ì—°ë™í•˜ì—¬ data ì €ì¥í•˜ê¸°](https://velog.io/@newnew_daddy/SPARK10)

â¬‡ï¸ ì†ŒìŠ¤ ë³€ê²½ â¬‡ï¸
1. âœ…produce í•˜ê¸°: smaple JSON íŒŒì¼ examplemsg.sh ì‹¤í–‰ 
2. âœ…spark ì„œë²„ì—ì„œ consume ì‹¤í–‰: ë°ì´í„° ê°€ê³µ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìˆ˜ì •í•œë‹¤. 
3. âœ…ì‹¤í–‰ ê°’ ë°›ì•„ì˜¤ê¸° 
4. âœ…data ì €ì¥í•˜ê¸° ( postgres )

# Sketch

1. airflow ì´ë¯¸ì§€ ë‹¤ì‹œ ë§ê¸° 
    1. sparkì— ëŒ€í•œ python package ë¹Œë“œì—… í•„ìš”
```txt
pandas~=2.0.3
requests~=2.31.0
selenium~=4.17.2
beautifulsoup4~=4.12.3
lxml~=5.1.0
kafka-python~=2.0.2
apache-airflow-providers-apache-kafka~=1.3.1
confluent-kafka~=2.3.0
+
add please
```

2. spark ì„œë²„ì—ì„œ ìë™ consume ì‹¤í–‰
> spark.py
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json
from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType

spark = SparkSession \
    .builder \
    .appName("Streaming from Kafka") \
    .config("spark.streaming.stopGracefullyOnShutdown", True) \
    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \
    .config("spark.sql.shuffle.partitions", 4) \
    .master("local[*]") \
    .getOrCreate()
# Define Kafka connection properties
kafka_params = {
    "kafka.bootstrap.servers": "125.6.40.186:19092",
    "subscribe": "devices",
    "startingOffsets": "earliest"
}

# Define JSON Schema
json_schema = StructType([
    StructField('eventId', StringType(), True),
    StructField('eventOffset', LongType(), True),
    StructField('eventPublisher', StringType(), True),
    StructField('customerId', StringType(), True),
    StructField('data', StructType([
        StructField('devices', ArrayType(StructType([
            StructField('deviceId', StringType(), True),
            StructField('temperature', LongType(), True),
            StructField('measure', StringType(), True),
            StructField('status', StringType(), True)
        ]), True), True)
    ]), True),
    StructField('eventTime', StringType(), True)
])

# Read Kafka messages
streaming_df = spark \
    .readStream \
    .format("kafka") \
    .options(**kafka_params) \
    .load()

# Parse JSON messages
json_df = streaming_df.selectExpr("CAST(value AS STRING) AS json") \
    .select(from_json("json", json_schema).alias("data")) \
    .select("data.*")

# Start streaming query
query = json_df \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

# Wait for the termination of the query
query.awaitTermination()
```

> device_events
```python
# Generate random events data
import datetime
import time
import uuid
import random
import json

event_status: list = ["SUCCESS", "ERROR", "STANDBY"] + [None]
device_id: list = ['D' + str(_id).rjust(3, '0') for _id in range(1, 6)] + [None]
customer_id: list = ["CI" + str(_id).rjust(5, '0') for _id in range(100, 121)]


# Generate event data from devices
def generate_events(offset=0):
    _event = {
        "eventId": str(uuid.uuid4()),
        "eventOffset": offset,
        "eventPublisher": "device",
        "customerId": random.choice(customer_id),
        "data": {
            "devices": [
                {
                    "deviceId": random.choice(device_id),
                    "temperature": random.randint(0, 30),
                    "measure": "C",
                    "status": random.choice(event_status)
                } for i in range(random.randint(0, 3))
            ],
        },
        "eventTime": str(datetime.datetime.now())
    }

    return json.dumps(_event)



if __name__ == "__main__":
    _offset = 10000
    while True:
        print(generate_events(offset=_offset))
        time.sleep(random.randint(0, 5))
        _offset += 1
```

> post-to-kafka.py
```python
# Method posts events to Kafka Server
# run command in kafka server to create topic : 
# ./usr/bin/kafka-topics --create --topic device_data --bootstrap-server kafka:9092 
from kafka import KafkaProducer, KafkaConsumer
import time
import random
from device_events import generate_events

__bootstrap_server = "kafka:29092"


def post_to_kafka(data):
    print('data: '+ str(data))
    producer = KafkaProducer(bootstrap_servers=__bootstrap_server)
    producer.send('devices', key=b'device', value=data)
    #producer.flush()
    producer.close()
    print("Posted to topic")


if __name__ == "__main__":
    _offset = 10000
    while True:
        post_to_kafka(bytes(str(generate_events(offset=_offset)), 'utf-8'))
        time.sleep(random.randint(0, 5))
        _offset += 1
```

> read_from_kafka.py
```python
# Method posts events to Kafka Server
from kafka import KafkaProducer, KafkaConsumer

__bootstrap_server = "kafka:29092"


def read_from_kafka():
    print("Reading through consumer")
    consumer = KafkaConsumer('devices', bootstrap_servers = __bootstrap_server)
    consumer.poll(timeout_ms=2000)
    for m in consumer:
        msg = str(m.value.decode('utf-8'))
        print('getting->' + msg)


if __name__ == "__main__":
    read_from_kafka()
```

3.  Kafka code ìˆ˜ì • 
> kafka consumeì„ spark consumeìœ¼ë¡œ ìˆ˜ì • 

```python
import os, sys

def produce_message():
  producer = KafkaProducer(bootstrap_servers=bootstrap_servers,linger_ms=20)
  meesage = os.system('source examplemsg.sh')
  producer.send('devices', message.encode())
  producer.flush

def consume_message():
SparkSubmitOperator
from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator
.
.
.
```

### ë°ì´í„° ê°€ê³µ ìŠ¤í¬ë¦½íŠ¸ ì¤‘ ERROR 

1. ê°€ê³µì´ ì˜ëª» ë¼ì„œ ê·¸ëŸ¼.. ã… 

```bash
pyspark.sql.utils.AnalysisException: Column 'customerId' does not exist. Did you mean one of the following? [devices];
'Project ['customerId, 'eventId, 'eventOffset, 'eventPublisher, 'eventTime, 'data]
+- Project [data#29.devices AS devices#37]
   +- Project [data#23.eventId AS eventId#25, data#23.eventOffset AS eventOffset#26L, data#23.eventPublisher AS eventPublisher#27, data#23.customerId AS customerId#28, data#23.data AS data#29, data#23.eventTime AS eventTime#30]
      +- Project [from_json(StructField(eventId,StringType,true), StructField(eventOffset,LongType,true), StructField(eventPublisher,StringType,true), StructField(customerId,StringType,true), StructField(data,StructType(StructField(devices,ArrayType(StructType(StructField(deviceId,StringType,true),StructField(temperature,LongType,true),StructField(measure,StringType,true),StructField(status,StringType,true)),true),true)),true), StructField(eventTime,StringType,true), json#21, Some(Etc/UTC)) AS data#23]
         +- Project [cast(value#8 as string) AS json#21]
            +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@59ed64f9, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2dd28f33, [startingOffsets=earliest, kafka.bootstrap.servers=125.6.40.186:19092, subscribe=devices], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3b89c20b,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 125.6.40.186:19092, subscribe -> devices, startingOffsets -> earliest),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]


```

##### ì½”ë“œ ê³µë¶€ 

```python
exploded_df.printSchema()
exploded_df.show
```

`spark.py`ì—ì„œ ë³´ë©´ ì €ëŸ° ì½”ë“œë“¤ë¡œ ì¶œë ¥í•˜ë©´ì„œ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ”ë° 

```
root
 |-- customerId: string (nullable = true)
 |-- eventId: string (nullable = true)
 |-- eventOffset: long (nullable = true)
 |-- eventPublisher: string (nullable = true)
 |-- eventTime: timestamp (nullable = true)
 |-- deviceId: string (nullable = true)
 |-- measure: string (nullable = true)
 |-- status: string (nullable = true)
 |-- temperature: long (nullable = true)
```

ìš”ê²ƒì´ ì € ì½”ë“œ ì¤‘ í•˜ë‚˜ë‹¤. 
ì´ê²Œ ì½”ë“œë¥¼ ì§„í–‰í•˜ë©´ì„œ ê°€ê³µëœ í˜•íƒœì¸ë° ì•„ë˜ì˜ í˜•ì‹ì²˜ëŸ¼ DataFrameì´ ì–´ë–»ê²Œ ë°”ë€Œì–´ ê°€ëŠ”ì§€, ì¦‰ Schema í˜•íƒœê°€ ì–´ë–»ê²Œ ë³€í˜•ë˜ì–´ ê°€ëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì½”ë“œì„! 

```bash
root
 |-- devices: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- deviceId: string (nullable = true)
 |    |    |-- measure: string (nullable = true)
 |    |    |-- status: string (nullable = true)
 |    |    |-- temperature: long (nullable = true)
```
â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸
```bash
root
 |-- customerId: string (nullable = true)
 |-- data: struct (nullable = true)
 |    |-- devices: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- deviceId: string (nullable = true)
 |    |    |    |-- measure: string (nullable = true)
 |    |    |    |-- status: string (nullable = true)
 |    |    |    |-- temperature: long (nullable = true)
 |-- eventId: string (nullable = true)
 |-- eventOffset: long (nullable = true)
 |-- eventPublisher: string (nullable = true)
 |-- eventTime: string (nullable = true)
```
â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸
```bash
root
 |-- customerId: string (nullable = true)
 |-- eventId: string (nullable = true)
 |-- eventOffset: long (nullable = true)
 |-- eventPublisher: string (nullable = true)
 |-- eventTime: string (nullable = true)
 |-- devices: struct (nullable = true)
 |    |-- deviceId: string (nullable = true)
 |    |-- measure: string (nullable = true)
 |    |-- status: string (nullable = true)
 |    |-- temperature: long (nullable = true)
```

{"eventId": "e3cb26d3-41b2-49a2-84f3-0156ed8d7502", "eventOffset": 10001, "eventPublisher": "device", "customerId": "CI00103", "data": {"devices": [{"deviceId": "D001", "temperature": 15, "measure": "C", "status": "ERROR"}, {"deviceId": "D002", "temperature": 16, "measure": "C", "status": "SUCCESS"}]}, "eventTime": "2023-01-05 11:13:53.643364"}

# Spark.py ë°ì´í„° ê°€ê³µ ìŠ¤í¬ë¦½íŠ¸ ìµœì¢…
#SPARK_PY

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json
from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType

spark = SparkSession \
    .builder \
    .appName("Streaming from Kafka") \
    .config("spark.streaming.stopGracefullyOnShutdown", True) \
    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \
    .config("spark.sql.shuffle.partitions", 4) \
    .master("local[*]") \
    .getOrCreate()
# Define Kafka connection properties
kafka_params = {
    "kafka.bootstrap.servers": "125.6.40.186:19092",
    "subscribe": "devices",
    "startingOffsets": "earliest"
}

# Define JSON Schema

json_schema = StructType([
    StructField('customerId', StringType(), True),
    StructField('data', StructType([
        StructField('devices', ArrayType(StructType([
            StructField('deviceId', StringType(), True),
            StructField('measure', StringType(), True),
            StructField('status', StringType(), True),
            StructField('temperature', LongType(), True)
        ]), True), True)
    ]), True),
    StructField('eventId', StringType(), True),
    StructField('eventOffset', LongType(), True),
    StructField('eventPublisher', StringType(), True),
    StructField('eventTime', StringType(), True)
])

# Read Kafka messages
streaming_df = spark \
    .readStream \
    .format("kafka") \
    .options(**kafka_params) \
    .load()

# Parse JSON messages
json_df = streaming_df.selectExpr("CAST(value AS STRING) AS value") \

json_expanded_df = json_df.withColumn("value", from_json(json_df["value"], json_schema)).select("value.*")

#print(json_schema)
#json_expanded_df.printSchema()

###############################################

from pyspark.sql.functions import explode, col


exploded_df = json_expanded_df \
    .select("customerId", "eventId", "eventOffset", "eventPublisher", "eventTime", "data") \
    .withColumn("devices", explode("data.devices")) \
    .drop("data")

#exploded_df.printSchema()
#exploded_df.show

flattened_df = exploded_df \
    .selectExpr("customerId", "eventId", "eventOffset", "eventPublisher", "cast(eventTime as timestamp) as eventTime",
                "devices.deviceId as deviceId", "devices.measure as measure",
                "devices.status as status", "devices.temperature as temperature")

#flattened_df.printSchema()


# Aggregate the dataframes to find the average temparature
# per Customer per device throughout the day for SUCCESS events
from pyspark.sql.functions import to_date, avg

agg_df = flattened_df.where("STATUS = 'SUCCESS'") \
    .withColumn("eventDate", to_date("eventTime", "yyyy-MM-dd")) \
    .groupBy("customerId","deviceId","eventDate") \
    .agg(avg("temperature").alias("avg_temp"))

# Write the output to console sink to check the output
writing_df = agg_df.writeStream \
    .format("console") \
    .option("checkpointLocation","checkpoint_dir") \
    .outputMode("complete") \
    .start()

# Start the streaming application to run until the following happens
# 1. Exception in the running program
# 2. Manual Interruption
writing_df.awaitTermination()


#query = flattened_df \
#    .writeStream \
#    .outputMode("append") \
#    .format("console") \
#    .start()

# Wait for the termination of the query
#query.awaitTermination()

```

# *AIRFLOW*
## airflow spark ì—°ë™

### airflowì˜ requirements.txtì— pip ì¶”ê°€
> `apache-airflow-providers-apache-spark==4.7.1`

### SPARK_HOME, JAVA_HOME í™˜ê²½ë³€ìˆ˜ ì²˜ë¦¬

#### ERROR: ![[Pasted image 20240228160941.png]]
: í•˜ì§€ë§Œ requirements.txtì— spark ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¥¼ ì´ë¯¸ í•œ ìƒíƒœ
ê·¸ë˜ì„œ ì´ ë•ŒëŠ” `pyspark` CLIë¥¼ ì°¾ì§€ ëª»í•´ì„œ ê·¸ë ‡ë‹¤ê³  í•¨.

1. ì»¨í…Œì´ë„ˆë¡œ ë“¤ì–´ê°€ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì¡ì•„ì¤Œ

```bash
sbin$ which pyspark
/home/airflow/.local/bin/pyspark
airflow@airflow-webserver-66d5f4b595-c4l2g:/sbin$ export SPARK_HOME=/home/airflow/.local/bin
airflow@airflow-webserver-66d5f4b595-c4l2g:/sbin$ export SPARK_HOME=/home/airflow/.local
airflow@airflow-webserver-66d5f4b595-c4l2g:/sbin$ export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
```

2. ê·¸ëŸ°ë° ì´ë²ˆì—ëŠ” JAVA_HOMEì´ ì—†ë‹¤ê³  ì—ëŸ¬
```
/home/airflow/.local/bin/load-spark-env.sh: line 68: ps: command not found
JAVA_HOME is not set
```

3. í™˜ê²½ ë³€ìˆ˜ë‚˜ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜ í•˜ë„ë¡ ë§Œë“¤ì–´ì§„ Dockerfileì„ ìˆ˜ì •í•´ì¤Œ
```Dockerfile
FROM apache/airflow:2.8.1

RUN echo $PATH

# Set AIRFLOW_HOME environment variable
ENV AIRFLOW_HOME=/home/airflow/.local/bin
# Set JAVA_HOME environment variable
ENV JAVA_HOME=/home/airflow/.local/bin

# Set SPARK_HOME environment variable
ENV SPARK_HOME=/home/airflow/.local/bin

# Set SPARK_HOME enviroment variable
ENV PATH="$PATH:${AIRFLOW_HOME}$:${JAVA_HOME}:${SPARK_HOME}"

RUN echo $PATH

USER root
RUN apt-get update \
  && apt-get install -y --no-install-recommends \
        vim \
        openjdk-17-jdk
COPY ./dags/*.py /opt/airflow/dags/
USER airflow
COPY requirements.txt /
RUN pip install --no-cache-dir "apache-airflow==2.8.1" -r /requirements.txt
```

```bash
docker build -t seunghyejeong/airflow:1.0 .
docker push seunghyejeong/airflow:1.0
```

#### ERROR You need to build Spark with the target "package" before running this program.
```bash
/home/airflow/.local/bin/load-spark-env.sh: line 68: ps: command not found
/home/airflow/.local/bin/load-spark-env.sh: line 68: ps: command not found
Failed to find Spark jars directory (/home/airflow/.local/assembly/target/scala-2.12/jars).
You need to build Spark with the target "package" before running this program.
```

ê·¸ë˜ë„ ì—¬ì „íˆ ì—ëŸ¬ê°€ ë‚˜ëŠ”ë° .. ,, [[workflow(240228)#âŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒ]]

### Airflow UIì—ì„œ Connection ë§Œë“¤ê¸° 

1. ìœ„ì˜ python íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¡œ Spark í•­ëª©ì´ ìƒê²¼ìŒ
    - connection type: Spark
    - Host: spark://master
    - port: 7077
### Docker image build í›„ airflow upgrade í•´ì£¼ê¸° 

```bash
helm upgrade airflow apache-airflow/airflow --namespace airflow -f custom_values.yaml
```

- ui
```
http://133.186.244.202:31151/
```

![[Pasted image 20240228151625.png]]


![[Pasted image 20240228151809.png]]
## pipeline dag ì‘ì„±

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from kafka import KafkaProducer, KafkaConsumer
from kafka.admin import KafkaAdminClient, NewTopic
from datetime import datetime
from airflow.models import TaskInstance
from kafka.errors import TopicAlreadyExistsError
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json
from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType

# Define Kafka broker
bootstrap_servers = '125.6.40.186:19092'
spark_master = 'bami-cluster2:7077'
topic_name = 'devices'


# Check with Kafka
def check_connected_kafka():
    try:
        producer = KafkaProducer(bootstrap_servers=bootstrap_servers)
        consumer = KafkaConsumer(topic_name, bootstrap_servers=bootstrap_servers)
        print(f"#####SUCCESS CONNECTING WITH KAFKA#######")
        return True
    except Exception as e:
        print(f"Error connecting to Kafka: {str(e)}")
        return False

# Check with Spark
def check_connedted_spark():
    try:
    spark = SparkSession \
        .builder \
        .appName("Streaming from Kafka") \
        .config("spark.streaming.stopGracefullyOnShutdown", True) \
        .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \
        .config("spark.sql.shuffle.partitions", 4) \
        .master(spark_master) \
        .getOrCreate()
        return True
    except Exception as e:
        print(f"FAIL..............FAIL SPARK........: {str{e}}")
        return False
# Define the DAG
default_args = {
    'owner': 'admin',
    'depends_on_past': False,
    'start_date': datetime(2024, 2, 27),
    'retries': 1,
}
dag = DAG(
    'airflow_kafka_spark_interworking',
    default_args=default_args,
    description='DAG for testing interworking of Airflow and Kafka and Spark',
    schedule_interval=None,
)

# Define tasks
check_connected_kafka_task = PythonOperator(
    task_id='check_connected_kafka',
    python_callable=check_connected_kafka,
    dag=dag,
)

check_connedted_spark_task = PythonOperator(
    task_id='check_connedted_spark',
    python_callable=check_connedted_spark,
    dag=dags,
)
```

# âŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒâŒ

**DOCKER FILEì´ë‚˜ SPAKR ê´€ë ¨ Packageë¥¼ ë¹Œë“œí•˜ëŠ”ê²Œ ì•„ë‹Œê±°ê°™ì•„ .............................
airflowì—ì„œëŠ” spark connectë§Œ í•˜ê³  
spark containerì—ì„œ Spark Session ì‹¤í–‰ í•˜ëŠ” ê±° ì•„ë‹ê¹Œ.;;;;;;;;;;;;;;;;;;;;;;;;*

ì •ë‹µì€ : [[workflow(240229)]]






```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from kafka import KafkaProducer, KafkaConsumer
from kafka.admin import KafkaAdminClient, NewTopic
from datetime import datetime
from airflow.models import TaskInstance
from kafka.errors import TopicAlreadyExistsError
from airflow.contrib.hooks.spark_submit_hook import SparkSubmitHook
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator 


# Define Kafka broker
bootstrap_servers = '125.6.40.186:19092'
spark_master = 'bami-cluster2:7077'
topic_name = 'devices'


# Check with Kafka
def check_connected_kafka():
    try:
        producer = KafkaProducer(bootstrap_servers=bootstrap_servers)
        consumer = KafkaConsumer(topic_name, bootstrap_servers=bootstrap_servers)
        print(f"#####SUCCESS CONNECTING WITH KAFKA#######")
        return True
    except Exception as e:
        print(f"Error connecting to Kafka: {str(e)}")
        return False

with DAG(dag_id='check_connected_kafka', start_date=datetime(2023, 1, 1)) as dag: 
    submit_spark_app = SparkSubmitOperator( 
        task_id='check_connected_kafka', 
        conn_id='spark_default', 
        application='/opt/bitnami/spark.py', 
        name='Streaming_from_Kafka'
    ) 

# Define the DAG
default_args = {
    'owner': 'admin',
    'depends_on_past': False,
    'start_date': datetime(2024, 2, 27),
    'retries': 1,
}
dag = DAG(
    'airflow_kafka_spark_interworking',
    default_args=default_args,
    description='DAG for testing interworking of Airflow and Kafka and Spark',
    schedule_interval=None,
)

# Define tasks
check_connected_kafka_task = PythonOperator(
    task_id='check_connected_kafka',
    python_callable=check_connected_kafka,
    dag=dag,
)

check_connedted_spark_task = PythonOperator(
    task_id='check_connedted_spark',
    python_callable=check_connedted_spark,
    dag=dags,
)


```